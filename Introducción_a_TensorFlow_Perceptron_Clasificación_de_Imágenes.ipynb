{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Introducción a TensorFlow_Perceptron Clasificación de Imágenes.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnIsAsPe/RNN-Resumenes-de-Texto/blob/main/Introducci%C3%B3n_a_TensorFlow_Perceptron_Clasificaci%C3%B3n_de_Im%C3%A1genes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec2m7RHibrJz"
      },
      "source": [
        "# Lectura de bibliotecas y Datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0trJmd6DjqBZ"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "import pandas as pd\r\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2dmuKS83QCC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "outputId": "3015b314-5824-4fa6-f76e-454646938356"
      },
      "source": [
        "datos = pd.read_csv('/content/drive/MyDrive/Datos/img_cancer_26x26pixeles_con_etiqueta.csv')\r\n",
        "\r\n",
        "print(datos.shape)\r\n",
        "datos.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5063, 677)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>637</th>\n",
              "      <th>638</th>\n",
              "      <th>639</th>\n",
              "      <th>640</th>\n",
              "      <th>641</th>\n",
              "      <th>642</th>\n",
              "      <th>643</th>\n",
              "      <th>644</th>\n",
              "      <th>645</th>\n",
              "      <th>646</th>\n",
              "      <th>647</th>\n",
              "      <th>648</th>\n",
              "      <th>649</th>\n",
              "      <th>650</th>\n",
              "      <th>651</th>\n",
              "      <th>652</th>\n",
              "      <th>653</th>\n",
              "      <th>654</th>\n",
              "      <th>655</th>\n",
              "      <th>656</th>\n",
              "      <th>657</th>\n",
              "      <th>658</th>\n",
              "      <th>659</th>\n",
              "      <th>660</th>\n",
              "      <th>661</th>\n",
              "      <th>662</th>\n",
              "      <th>663</th>\n",
              "      <th>664</th>\n",
              "      <th>665</th>\n",
              "      <th>666</th>\n",
              "      <th>667</th>\n",
              "      <th>668</th>\n",
              "      <th>669</th>\n",
              "      <th>670</th>\n",
              "      <th>671</th>\n",
              "      <th>672</th>\n",
              "      <th>673</th>\n",
              "      <th>674</th>\n",
              "      <th>675</th>\n",
              "      <th>clase</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083225</td>\n",
              "      <td>0.083405</td>\n",
              "      <td>0.086613</td>\n",
              "      <td>0.094121</td>\n",
              "      <td>0.095760</td>\n",
              "      <td>0.095939</td>\n",
              "      <td>0.095679</td>\n",
              "      <td>0.095800</td>\n",
              "      <td>0.096059</td>\n",
              "      <td>0.095816</td>\n",
              "      <td>0.097095</td>\n",
              "      <td>0.099579</td>\n",
              "      <td>0.104371</td>\n",
              "      <td>0.121509</td>\n",
              "      <td>0.151148</td>\n",
              "      <td>0.166485</td>\n",
              "      <td>0.142229</td>\n",
              "      <td>0.100965</td>\n",
              "      <td>0.084538</td>\n",
              "      <td>0.083237</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083240</td>\n",
              "      <td>0.084179</td>\n",
              "      <td>0.092702</td>\n",
              "      <td>0.113767</td>\n",
              "      <td>0.125615</td>\n",
              "      <td>0.128518</td>\n",
              "      <td>0.128971</td>\n",
              "      <td>0.129441</td>\n",
              "      <td>0.130263</td>\n",
              "      <td>0.130819</td>\n",
              "      <td>0.130473</td>\n",
              "      <td>0.130028</td>\n",
              "      <td>...</td>\n",
              "      <td>0.188656</td>\n",
              "      <td>0.188316</td>\n",
              "      <td>0.192373</td>\n",
              "      <td>0.195840</td>\n",
              "      <td>0.194437</td>\n",
              "      <td>0.196929</td>\n",
              "      <td>0.193218</td>\n",
              "      <td>0.175780</td>\n",
              "      <td>0.128152</td>\n",
              "      <td>0.089850</td>\n",
              "      <td>0.083352</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083259</td>\n",
              "      <td>0.085485</td>\n",
              "      <td>0.109178</td>\n",
              "      <td>0.145949</td>\n",
              "      <td>0.147400</td>\n",
              "      <td>0.147373</td>\n",
              "      <td>0.153215</td>\n",
              "      <td>0.136331</td>\n",
              "      <td>0.128650</td>\n",
              "      <td>0.127448</td>\n",
              "      <td>0.124526</td>\n",
              "      <td>0.123044</td>\n",
              "      <td>0.123262</td>\n",
              "      <td>0.126940</td>\n",
              "      <td>0.126231</td>\n",
              "      <td>0.126898</td>\n",
              "      <td>0.121795</td>\n",
              "      <td>0.099420</td>\n",
              "      <td>0.084719</td>\n",
              "      <td>0.083252</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083241</td>\n",
              "      <td>0.084892</td>\n",
              "      <td>0.112470</td>\n",
              "      <td>0.192035</td>\n",
              "      <td>0.219843</td>\n",
              "      <td>0.187999</td>\n",
              "      <td>0.180500</td>\n",
              "      <td>0.186738</td>\n",
              "      <td>0.186549</td>\n",
              "      <td>0.189735</td>\n",
              "      <td>0.188186</td>\n",
              "      <td>0.190577</td>\n",
              "      <td>0.193022</td>\n",
              "      <td>0.192217</td>\n",
              "      <td>0.186904</td>\n",
              "      <td>0.186196</td>\n",
              "      <td>0.171629</td>\n",
              "      <td>0.112510</td>\n",
              "      <td>0.085044</td>\n",
              "      <td>0.083240</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083305</td>\n",
              "      <td>0.089308</td>\n",
              "      <td>0.159444</td>\n",
              "      <td>0.323082</td>\n",
              "      <td>0.435133</td>\n",
              "      <td>0.479868</td>\n",
              "      <td>0.425607</td>\n",
              "      <td>0.406373</td>\n",
              "      <td>0.416591</td>\n",
              "      <td>0.418254</td>\n",
              "      <td>0.416060</td>\n",
              "      <td>0.420284</td>\n",
              "      <td>...</td>\n",
              "      <td>0.350737</td>\n",
              "      <td>0.328330</td>\n",
              "      <td>0.313045</td>\n",
              "      <td>0.315160</td>\n",
              "      <td>0.312431</td>\n",
              "      <td>0.316695</td>\n",
              "      <td>0.318328</td>\n",
              "      <td>0.278893</td>\n",
              "      <td>0.166274</td>\n",
              "      <td>0.092502</td>\n",
              "      <td>0.083354</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083301</td>\n",
              "      <td>0.089128</td>\n",
              "      <td>0.159247</td>\n",
              "      <td>0.262119</td>\n",
              "      <td>0.234541</td>\n",
              "      <td>0.216824</td>\n",
              "      <td>0.201565</td>\n",
              "      <td>0.194039</td>\n",
              "      <td>0.190225</td>\n",
              "      <td>0.190590</td>\n",
              "      <td>0.191628</td>\n",
              "      <td>0.185784</td>\n",
              "      <td>0.175968</td>\n",
              "      <td>0.180713</td>\n",
              "      <td>0.180095</td>\n",
              "      <td>0.184120</td>\n",
              "      <td>0.171356</td>\n",
              "      <td>0.119763</td>\n",
              "      <td>0.086424</td>\n",
              "      <td>0.083276</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083240</td>\n",
              "      <td>0.084891</td>\n",
              "      <td>0.108390</td>\n",
              "      <td>0.155804</td>\n",
              "      <td>0.162704</td>\n",
              "      <td>0.179060</td>\n",
              "      <td>0.174836</td>\n",
              "      <td>0.160696</td>\n",
              "      <td>0.163860</td>\n",
              "      <td>0.181359</td>\n",
              "      <td>0.174879</td>\n",
              "      <td>0.160358</td>\n",
              "      <td>0.162727</td>\n",
              "      <td>0.175161</td>\n",
              "      <td>0.189507</td>\n",
              "      <td>0.215191</td>\n",
              "      <td>0.226223</td>\n",
              "      <td>0.136367</td>\n",
              "      <td>0.086611</td>\n",
              "      <td>0.083256</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083272</td>\n",
              "      <td>0.087915</td>\n",
              "      <td>0.154537</td>\n",
              "      <td>0.309475</td>\n",
              "      <td>0.352005</td>\n",
              "      <td>0.328503</td>\n",
              "      <td>0.358213</td>\n",
              "      <td>0.373566</td>\n",
              "      <td>0.346785</td>\n",
              "      <td>0.339945</td>\n",
              "      <td>0.390837</td>\n",
              "      <td>0.392600</td>\n",
              "      <td>...</td>\n",
              "      <td>0.332288</td>\n",
              "      <td>0.331581</td>\n",
              "      <td>0.343050</td>\n",
              "      <td>0.345295</td>\n",
              "      <td>0.330323</td>\n",
              "      <td>0.323152</td>\n",
              "      <td>0.320105</td>\n",
              "      <td>0.279885</td>\n",
              "      <td>0.169279</td>\n",
              "      <td>0.092595</td>\n",
              "      <td>0.083367</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083344</td>\n",
              "      <td>0.091139</td>\n",
              "      <td>0.171438</td>\n",
              "      <td>0.296291</td>\n",
              "      <td>0.282717</td>\n",
              "      <td>0.251271</td>\n",
              "      <td>0.217253</td>\n",
              "      <td>0.197555</td>\n",
              "      <td>0.193422</td>\n",
              "      <td>0.184207</td>\n",
              "      <td>0.180436</td>\n",
              "      <td>0.170264</td>\n",
              "      <td>0.177799</td>\n",
              "      <td>0.194378</td>\n",
              "      <td>0.195019</td>\n",
              "      <td>0.178956</td>\n",
              "      <td>0.169096</td>\n",
              "      <td>0.118785</td>\n",
              "      <td>0.086534</td>\n",
              "      <td>0.083280</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083241</td>\n",
              "      <td>0.084968</td>\n",
              "      <td>0.108683</td>\n",
              "      <td>0.163460</td>\n",
              "      <td>0.192673</td>\n",
              "      <td>0.178441</td>\n",
              "      <td>0.165327</td>\n",
              "      <td>0.172069</td>\n",
              "      <td>0.186495</td>\n",
              "      <td>0.178956</td>\n",
              "      <td>0.167983</td>\n",
              "      <td>0.162857</td>\n",
              "      <td>0.169700</td>\n",
              "      <td>0.184739</td>\n",
              "      <td>0.207286</td>\n",
              "      <td>0.224597</td>\n",
              "      <td>0.247589</td>\n",
              "      <td>0.140882</td>\n",
              "      <td>0.086850</td>\n",
              "      <td>0.083257</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083273</td>\n",
              "      <td>0.088753</td>\n",
              "      <td>0.156897</td>\n",
              "      <td>0.300279</td>\n",
              "      <td>0.347244</td>\n",
              "      <td>0.385530</td>\n",
              "      <td>0.391457</td>\n",
              "      <td>0.352362</td>\n",
              "      <td>0.352138</td>\n",
              "      <td>0.404086</td>\n",
              "      <td>0.410804</td>\n",
              "      <td>0.362560</td>\n",
              "      <td>...</td>\n",
              "      <td>0.364815</td>\n",
              "      <td>0.359175</td>\n",
              "      <td>0.341390</td>\n",
              "      <td>0.352469</td>\n",
              "      <td>0.337021</td>\n",
              "      <td>0.346255</td>\n",
              "      <td>0.342502</td>\n",
              "      <td>0.302029</td>\n",
              "      <td>0.177949</td>\n",
              "      <td>0.093565</td>\n",
              "      <td>0.083382</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083307</td>\n",
              "      <td>0.089709</td>\n",
              "      <td>0.165956</td>\n",
              "      <td>0.278964</td>\n",
              "      <td>0.248794</td>\n",
              "      <td>0.222636</td>\n",
              "      <td>0.206214</td>\n",
              "      <td>0.199162</td>\n",
              "      <td>0.194352</td>\n",
              "      <td>0.190506</td>\n",
              "      <td>0.195654</td>\n",
              "      <td>0.206230</td>\n",
              "      <td>0.195324</td>\n",
              "      <td>0.191298</td>\n",
              "      <td>0.182867</td>\n",
              "      <td>0.184448</td>\n",
              "      <td>0.180449</td>\n",
              "      <td>0.122997</td>\n",
              "      <td>0.086934</td>\n",
              "      <td>0.083285</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083243</td>\n",
              "      <td>0.085150</td>\n",
              "      <td>0.125502</td>\n",
              "      <td>0.223968</td>\n",
              "      <td>0.188510</td>\n",
              "      <td>0.181894</td>\n",
              "      <td>0.181329</td>\n",
              "      <td>0.172335</td>\n",
              "      <td>0.169007</td>\n",
              "      <td>0.173832</td>\n",
              "      <td>0.184832</td>\n",
              "      <td>0.188771</td>\n",
              "      <td>0.204627</td>\n",
              "      <td>0.220360</td>\n",
              "      <td>0.238141</td>\n",
              "      <td>0.270100</td>\n",
              "      <td>0.229637</td>\n",
              "      <td>0.128614</td>\n",
              "      <td>0.086290</td>\n",
              "      <td>0.083253</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083290</td>\n",
              "      <td>0.089593</td>\n",
              "      <td>0.165005</td>\n",
              "      <td>0.366662</td>\n",
              "      <td>0.548632</td>\n",
              "      <td>0.433108</td>\n",
              "      <td>0.428869</td>\n",
              "      <td>0.439226</td>\n",
              "      <td>0.422466</td>\n",
              "      <td>0.382484</td>\n",
              "      <td>0.364961</td>\n",
              "      <td>0.389379</td>\n",
              "      <td>...</td>\n",
              "      <td>0.356768</td>\n",
              "      <td>0.350259</td>\n",
              "      <td>0.345512</td>\n",
              "      <td>0.349427</td>\n",
              "      <td>0.360185</td>\n",
              "      <td>0.365872</td>\n",
              "      <td>0.349878</td>\n",
              "      <td>0.325673</td>\n",
              "      <td>0.188289</td>\n",
              "      <td>0.094558</td>\n",
              "      <td>0.083407</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083289</td>\n",
              "      <td>0.087291</td>\n",
              "      <td>0.130107</td>\n",
              "      <td>0.201024</td>\n",
              "      <td>0.221397</td>\n",
              "      <td>0.227356</td>\n",
              "      <td>0.207675</td>\n",
              "      <td>0.190300</td>\n",
              "      <td>0.199108</td>\n",
              "      <td>0.204295</td>\n",
              "      <td>0.192606</td>\n",
              "      <td>0.190277</td>\n",
              "      <td>0.187772</td>\n",
              "      <td>0.186235</td>\n",
              "      <td>0.187462</td>\n",
              "      <td>0.198557</td>\n",
              "      <td>0.191787</td>\n",
              "      <td>0.125617</td>\n",
              "      <td>0.087338</td>\n",
              "      <td>0.083290</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 677 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          0         1         2         3  ...       673       674       675  clase\n",
              "0  0.083223  0.083223  0.083223  0.083225  ...  0.083223  0.083223  0.083223      0\n",
              "1  0.083223  0.083223  0.083223  0.083241  ...  0.083223  0.083223  0.083223      0\n",
              "2  0.083223  0.083223  0.083223  0.083240  ...  0.083223  0.083223  0.083223      0\n",
              "3  0.083223  0.083223  0.083223  0.083241  ...  0.083223  0.083223  0.083223      0\n",
              "4  0.083223  0.083223  0.083223  0.083243  ...  0.083223  0.083223  0.083223      0\n",
              "\n",
              "[5 rows x 677 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQZCWzLks2Sj"
      },
      "source": [
        "X = datos.drop('clase', axis=1)\r\n",
        "Y = datos['clase']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xtxnY3PtILA"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\r\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, \r\n",
        "                                                    shuffle=True, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMXmutUb2m-t"
      },
      "source": [
        "# 1. Construcción del modelo "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPZ68wASog_I"
      },
      "source": [
        "* [Modelo secuencial de Keras](https://keras.io/guides/sequential_model/)\r\n",
        "* [Funciones de activación ](https://keras.io/api/layers/activations/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3IKyzTCDNGo"
      },
      "source": [
        "model = tf.keras.models.Sequential([               \n",
        "\n",
        "  tf.keras.layers.Dense(1,                         \n",
        "                        activation='sigmoid',\n",
        "                        input_shape = (676,)\n",
        "                        )\n",
        "\n",
        "  #Dropout previene el sobreajuste del modelo haciendo 0 un porcentaje de las entradas.\n",
        "  #tf.keras.layers.Dropout(0.2),  \n",
        "\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYBtK0CqY_jM",
        "outputId": "e36202f1-1dd2-4d6c-e272-070afece81c6"
      },
      "source": [
        "#Regresa una lista de las capas del modelo\r\n",
        "model.layers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tensorflow.python.keras.layers.core.Dense at 0x7fbb2c715128>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwTT7x5bbhxD",
        "outputId": "572a760b-104e-496c-83fa-5aeaaeb917bc"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 1)                 677       \n",
            "=================================================================\n",
            "Total params: 677\n",
            "Trainable params: 677\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eT3z0t0_aZwa",
        "outputId": "7e98d7e2-b9d8-4192-86a4-afd2512db174"
      },
      "source": [
        "model.add(tf.keras.layers.Dense(3,\r\n",
        "                                activation='relu'))\r\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 1)                 677       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 3)                 6         \n",
            "=================================================================\n",
            "Total params: 683\n",
            "Trainable params: 683\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqHHVV1ANtsQ",
        "outputId": "def6d5b9-a06e-47e2-b604-549da540f118"
      },
      "source": [
        "model.pop()\r\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 1)                 677       \n",
            "=================================================================\n",
            "Total params: 677\n",
            "Trainable params: 677\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        },
        "id": "BwvdM7vTO2m0",
        "outputId": "78bf92ea-ae9f-4574-de3c-db8a74f27934"
      },
      "source": [
        "tf.keras.utils.plot_model( \r\n",
        "    model,\r\n",
        "    to_file=\"model.png\",\r\n",
        "    show_shapes=False,\r\n",
        "    show_dtype=False,\r\n",
        "    show_layer_names=True,\r\n",
        "    rankdir=\"LR\",\r\n",
        "    dpi=96,\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEj4kqqjNfRW"
      },
      "source": [
        "# Entrenar modelo\r\n",
        "\r\n",
        "Compilar el modelo eligiendo un optimizador"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXPqMdR4NbeQ"
      },
      "source": [
        "model.compile(loss='binary_crossentropy',       # función objetivo  que se busca minimizar\r\n",
        "                                                # https://keras.io/api/losses/\r\n",
        "              \r\n",
        "              optimizer='adam',                 # stochastic gradient descent\r\n",
        "                                                # https://keras.io/api/optimizers/adam/\r\n",
        "              \r\n",
        "              metrics=['accuracy'])             \r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81D05Qk_UoCZ",
        "outputId": "883b9407-d587-419a-93eb-b18b6e576021"
      },
      "source": [
        "history = model.fit(x_train, y_train,    \r\n",
        "                    epochs=60,                 \r\n",
        "                    batch_size=25,       \r\n",
        "                    verbose = 2,       \r\n",
        "                    validation_data = (x_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "142/142 - 1s - loss: 0.4899 - accuracy: 0.7587 - val_loss: 0.5100 - val_accuracy: 0.7564\n",
            "Epoch 2/60\n",
            "142/142 - 0s - loss: 0.4933 - accuracy: 0.7602 - val_loss: 0.4914 - val_accuracy: 0.7485\n",
            "Epoch 3/60\n",
            "142/142 - 0s - loss: 0.4913 - accuracy: 0.7607 - val_loss: 0.4894 - val_accuracy: 0.7531\n",
            "Epoch 4/60\n",
            "142/142 - 0s - loss: 0.4912 - accuracy: 0.7650 - val_loss: 0.4905 - val_accuracy: 0.7479\n",
            "Epoch 5/60\n",
            "142/142 - 0s - loss: 0.4898 - accuracy: 0.7571 - val_loss: 0.4884 - val_accuracy: 0.7492\n",
            "Epoch 6/60\n",
            "142/142 - 0s - loss: 0.4896 - accuracy: 0.7579 - val_loss: 0.4883 - val_accuracy: 0.7505\n",
            "Epoch 7/60\n",
            "142/142 - 0s - loss: 0.4925 - accuracy: 0.7568 - val_loss: 0.4932 - val_accuracy: 0.7505\n",
            "Epoch 8/60\n",
            "142/142 - 0s - loss: 0.4881 - accuracy: 0.7590 - val_loss: 0.4927 - val_accuracy: 0.7577\n",
            "Epoch 9/60\n",
            "142/142 - 0s - loss: 0.4906 - accuracy: 0.7559 - val_loss: 0.4901 - val_accuracy: 0.7465\n",
            "Epoch 10/60\n",
            "142/142 - 0s - loss: 0.4889 - accuracy: 0.7593 - val_loss: 0.5087 - val_accuracy: 0.7531\n",
            "Epoch 11/60\n",
            "142/142 - 0s - loss: 0.4890 - accuracy: 0.7599 - val_loss: 0.4937 - val_accuracy: 0.7531\n",
            "Epoch 12/60\n",
            "142/142 - 0s - loss: 0.4864 - accuracy: 0.7655 - val_loss: 0.4941 - val_accuracy: 0.7551\n",
            "Epoch 13/60\n",
            "142/142 - 0s - loss: 0.4885 - accuracy: 0.7571 - val_loss: 0.5019 - val_accuracy: 0.7564\n",
            "Epoch 14/60\n",
            "142/142 - 0s - loss: 0.4889 - accuracy: 0.7576 - val_loss: 0.4938 - val_accuracy: 0.7577\n",
            "Epoch 15/60\n",
            "142/142 - 0s - loss: 0.4886 - accuracy: 0.7582 - val_loss: 0.4890 - val_accuracy: 0.7498\n",
            "Epoch 16/60\n",
            "142/142 - 0s - loss: 0.4874 - accuracy: 0.7610 - val_loss: 0.4889 - val_accuracy: 0.7492\n",
            "Epoch 17/60\n",
            "142/142 - 0s - loss: 0.4853 - accuracy: 0.7627 - val_loss: 0.4885 - val_accuracy: 0.7564\n",
            "Epoch 18/60\n",
            "142/142 - 0s - loss: 0.4838 - accuracy: 0.7599 - val_loss: 0.4887 - val_accuracy: 0.7498\n",
            "Epoch 19/60\n",
            "142/142 - 0s - loss: 0.4857 - accuracy: 0.7604 - val_loss: 0.4880 - val_accuracy: 0.7512\n",
            "Epoch 20/60\n",
            "142/142 - 0s - loss: 0.4850 - accuracy: 0.7613 - val_loss: 0.4864 - val_accuracy: 0.7472\n",
            "Epoch 21/60\n",
            "142/142 - 0s - loss: 0.4858 - accuracy: 0.7661 - val_loss: 0.4864 - val_accuracy: 0.7472\n",
            "Epoch 22/60\n",
            "142/142 - 0s - loss: 0.4842 - accuracy: 0.7604 - val_loss: 0.4880 - val_accuracy: 0.7498\n",
            "Epoch 23/60\n",
            "142/142 - 0s - loss: 0.4847 - accuracy: 0.7610 - val_loss: 0.4865 - val_accuracy: 0.7472\n",
            "Epoch 24/60\n",
            "142/142 - 0s - loss: 0.4829 - accuracy: 0.7644 - val_loss: 0.5077 - val_accuracy: 0.7525\n",
            "Epoch 25/60\n",
            "142/142 - 0s - loss: 0.4903 - accuracy: 0.7633 - val_loss: 0.4860 - val_accuracy: 0.7498\n",
            "Epoch 26/60\n",
            "142/142 - 0s - loss: 0.4861 - accuracy: 0.7669 - val_loss: 0.4918 - val_accuracy: 0.7571\n",
            "Epoch 27/60\n",
            "142/142 - 0s - loss: 0.4832 - accuracy: 0.7610 - val_loss: 0.4874 - val_accuracy: 0.7498\n",
            "Epoch 28/60\n",
            "142/142 - 0s - loss: 0.4848 - accuracy: 0.7624 - val_loss: 0.4854 - val_accuracy: 0.7485\n",
            "Epoch 29/60\n",
            "142/142 - 0s - loss: 0.4822 - accuracy: 0.7630 - val_loss: 0.5030 - val_accuracy: 0.7544\n",
            "Epoch 30/60\n",
            "142/142 - 0s - loss: 0.4825 - accuracy: 0.7630 - val_loss: 0.4970 - val_accuracy: 0.7577\n",
            "Epoch 31/60\n",
            "142/142 - 0s - loss: 0.4822 - accuracy: 0.7681 - val_loss: 0.4925 - val_accuracy: 0.7577\n",
            "Epoch 32/60\n",
            "142/142 - 0s - loss: 0.4808 - accuracy: 0.7627 - val_loss: 0.4964 - val_accuracy: 0.7558\n",
            "Epoch 33/60\n",
            "142/142 - 0s - loss: 0.4804 - accuracy: 0.7621 - val_loss: 0.4850 - val_accuracy: 0.7498\n",
            "Epoch 34/60\n",
            "142/142 - 0s - loss: 0.4831 - accuracy: 0.7675 - val_loss: 0.4854 - val_accuracy: 0.7505\n",
            "Epoch 35/60\n",
            "142/142 - 0s - loss: 0.4832 - accuracy: 0.7613 - val_loss: 0.4932 - val_accuracy: 0.7597\n",
            "Epoch 36/60\n",
            "142/142 - 0s - loss: 0.4856 - accuracy: 0.7664 - val_loss: 0.4840 - val_accuracy: 0.7518\n",
            "Epoch 37/60\n",
            "142/142 - 0s - loss: 0.4795 - accuracy: 0.7644 - val_loss: 0.4868 - val_accuracy: 0.7479\n",
            "Epoch 38/60\n",
            "142/142 - 0s - loss: 0.4831 - accuracy: 0.7647 - val_loss: 0.4856 - val_accuracy: 0.7531\n",
            "Epoch 39/60\n",
            "142/142 - 0s - loss: 0.4794 - accuracy: 0.7666 - val_loss: 0.4943 - val_accuracy: 0.7544\n",
            "Epoch 40/60\n",
            "142/142 - 0s - loss: 0.4782 - accuracy: 0.7633 - val_loss: 0.4974 - val_accuracy: 0.7584\n",
            "Epoch 41/60\n",
            "142/142 - 0s - loss: 0.4827 - accuracy: 0.7658 - val_loss: 0.4840 - val_accuracy: 0.7492\n",
            "Epoch 42/60\n",
            "142/142 - 0s - loss: 0.4803 - accuracy: 0.7678 - val_loss: 0.4835 - val_accuracy: 0.7512\n",
            "Epoch 43/60\n",
            "142/142 - 0s - loss: 0.4772 - accuracy: 0.7624 - val_loss: 0.4967 - val_accuracy: 0.7571\n",
            "Epoch 44/60\n",
            "142/142 - 0s - loss: 0.4777 - accuracy: 0.7647 - val_loss: 0.4854 - val_accuracy: 0.7538\n",
            "Epoch 45/60\n",
            "142/142 - 0s - loss: 0.4825 - accuracy: 0.7587 - val_loss: 0.4986 - val_accuracy: 0.7584\n",
            "Epoch 46/60\n",
            "142/142 - 0s - loss: 0.4799 - accuracy: 0.7613 - val_loss: 0.4900 - val_accuracy: 0.7577\n",
            "Epoch 47/60\n",
            "142/142 - 0s - loss: 0.4781 - accuracy: 0.7672 - val_loss: 0.4836 - val_accuracy: 0.7505\n",
            "Epoch 48/60\n",
            "142/142 - 0s - loss: 0.4789 - accuracy: 0.7621 - val_loss: 0.4833 - val_accuracy: 0.7492\n",
            "Epoch 49/60\n",
            "142/142 - 0s - loss: 0.4769 - accuracy: 0.7666 - val_loss: 0.4827 - val_accuracy: 0.7505\n",
            "Epoch 50/60\n",
            "142/142 - 0s - loss: 0.4777 - accuracy: 0.7664 - val_loss: 0.4825 - val_accuracy: 0.7505\n",
            "Epoch 51/60\n",
            "142/142 - 0s - loss: 0.4834 - accuracy: 0.7641 - val_loss: 0.4980 - val_accuracy: 0.7571\n",
            "Epoch 52/60\n",
            "142/142 - 0s - loss: 0.4784 - accuracy: 0.7650 - val_loss: 0.4901 - val_accuracy: 0.7551\n",
            "Epoch 53/60\n",
            "142/142 - 0s - loss: 0.4766 - accuracy: 0.7672 - val_loss: 0.4822 - val_accuracy: 0.7512\n",
            "Epoch 54/60\n",
            "142/142 - 0s - loss: 0.4778 - accuracy: 0.7695 - val_loss: 0.4824 - val_accuracy: 0.7512\n",
            "Epoch 55/60\n",
            "142/142 - 0s - loss: 0.4762 - accuracy: 0.7650 - val_loss: 0.4839 - val_accuracy: 0.7531\n",
            "Epoch 56/60\n",
            "142/142 - 0s - loss: 0.4746 - accuracy: 0.7661 - val_loss: 0.4865 - val_accuracy: 0.7558\n",
            "Epoch 57/60\n",
            "142/142 - 0s - loss: 0.4790 - accuracy: 0.7638 - val_loss: 0.4914 - val_accuracy: 0.7558\n",
            "Epoch 58/60\n",
            "142/142 - 0s - loss: 0.4744 - accuracy: 0.7683 - val_loss: 0.4818 - val_accuracy: 0.7512\n",
            "Epoch 59/60\n",
            "142/142 - 0s - loss: 0.4767 - accuracy: 0.7675 - val_loss: 0.4818 - val_accuracy: 0.7512\n",
            "Epoch 60/60\n",
            "142/142 - 0s - loss: 0.4743 - accuracy: 0.7712 - val_loss: 0.4889 - val_accuracy: 0.7551\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7dTAzgHDUh7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e5bfd82-3466-4821-9456-b00eb0c77b9b"
      },
      "source": [
        "model.evaluate(x_test,  y_test, verbose=2)\r\n",
        "print('Test loss:', score[0])\r\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "48/48 - 0s - loss: 0.4889 - accuracy: 0.7551\n",
            "Test loss: 0.4889371395111084\n",
            "Test accuracy: 0.7551020383834839\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}