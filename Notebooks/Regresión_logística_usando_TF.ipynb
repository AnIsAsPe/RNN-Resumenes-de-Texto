{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Regresión logística usando TF.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMfcyxJDwddnqe5taqIpUIV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnIsAsPe/RNN-Resumenes-de-Texto/blob/main/Notebooks/Regresi%C3%B3n_log%C3%ADstica_usando_TF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-URr6ioO31Je"
      },
      "source": [
        "# Cargar Bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXjUiWkiUQQA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a42b9fd1-b998-4989-8ce3-0bb55c438f75"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "R_qNtUcq4MQN",
        "outputId": "428dee38-101b-4699-aa8c-3fa88b4f83fb"
      },
      "source": [
        "datos = pd.read_csv('https://raw.githubusercontent.com/AnIsAsPe/LogisticRegression_SpamOpinion/master/Datos/deceptive-opinion.csv',\n",
        "                    usecols=['deceptive','text']\n",
        "                    )\n",
        "print(datos.shape)\n",
        "datos.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1600, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>deceptive</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>truthful</td>\n",
              "      <td>We stayed for a one night getaway with family ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>truthful</td>\n",
              "      <td>Triple A rate with upgrade to view room was le...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>truthful</td>\n",
              "      <td>This comes a little late as I'm finally catchi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>truthful</td>\n",
              "      <td>The Omni Chicago really delivers on all fronts...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>truthful</td>\n",
              "      <td>I asked for a high floor away from the elevato...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  deceptive                                               text\n",
              "0  truthful  We stayed for a one night getaway with family ...\n",
              "1  truthful  Triple A rate with upgrade to view room was le...\n",
              "2  truthful  This comes a little late as I'm finally catchi...\n",
              "3  truthful  The Omni Chicago really delivers on all fronts...\n",
              "4  truthful  I asked for a high floor away from the elevato..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WEq5tt2N41nW",
        "outputId": "310b65f5-3bdc-4b82-f276-abe90eebd339"
      },
      "source": [
        "#datos['polarity'].value_counts()\n",
        "datos['deceptive'].value_counts()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "truthful     800\n",
              "deceptive    800\n",
              "Name: deceptive, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZcQidulQNd0d",
        "outputId": "2488061b-a03b-48ad-a7c6-a7d3a1bdff34"
      },
      "source": [
        "datos['deceptive'] = np.where(datos['deceptive']=='deceptive', 1, 0)\n",
        "datos['deceptive'].value_counts()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    800\n",
              "0    800\n",
              "Name: deceptive, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B10uYOdPAm2-"
      },
      "source": [
        "## Preprocesamiento de textos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7k_x2WD3-iZ6"
      },
      "source": [
        "def preprocesar(texto):\n",
        "  #convierte a minúsculas\n",
        "  texto = (texto).lower()\n",
        "\n",
        "  #elimina stopwords\n",
        "  stop = re.compile(r'\\b(' + r'|'.join(stopwords.words('english')) + r')\\b\\s*')\n",
        "  texto = stop.sub('', texto) \n",
        "\n",
        "  #quita puntuaciones y números\n",
        "  texto = re.sub('[^a-z]+', ' ', texto)\n",
        "\n",
        "  #lematizar y quedarnos con palabras que tengan más de tres caracteres\n",
        "  st = PorterStemmer()\n",
        "  texto = texto.split()\n",
        "  texto = ' '.join([st.stem(i) for i in texto])\n",
        "  \n",
        "  return(texto)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "d5NaISfg6op2",
        "outputId": "8d328cae-5ade-491f-e8eb-44a7aebf62ab"
      },
      "source": [
        "datos['text_pp'] = datos['text'].apply(preprocesar)\n",
        "datos"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>deceptive</th>\n",
              "      <th>text</th>\n",
              "      <th>text_pp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>We stayed for a one night getaway with family ...</td>\n",
              "      <td>stay one night getaway famili thursday tripl a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>Triple A rate with upgrade to view room was le...</td>\n",
              "      <td>tripl rate upgrad view room less also includ b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>This comes a little late as I'm finally catchi...</td>\n",
              "      <td>come littl late final catch review past sever ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>The Omni Chicago really delivers on all fronts...</td>\n",
              "      <td>omni chicago realli deliv front spacious room ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>I asked for a high floor away from the elevato...</td>\n",
              "      <td>ask high floor away elev got room pleasantli d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1595</th>\n",
              "      <td>1</td>\n",
              "      <td>Problems started when I booked the InterContin...</td>\n",
              "      <td>problem start book intercontinent chicago onli...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1596</th>\n",
              "      <td>1</td>\n",
              "      <td>The Amalfi Hotel has a beautiful website and i...</td>\n",
              "      <td>amalfi hotel beauti websit interior decor wife...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1597</th>\n",
              "      <td>1</td>\n",
              "      <td>The Intercontinental Chicago Magnificent Mile ...</td>\n",
              "      <td>intercontinent chicago magnific mile outsid ho...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1598</th>\n",
              "      <td>1</td>\n",
              "      <td>The Palmer House Hilton, while it looks good i...</td>\n",
              "      <td>palmer hous hilton look good pictur outsid act...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599</th>\n",
              "      <td>1</td>\n",
              "      <td>As a former Chicagoan, I'm appalled at the Ama...</td>\n",
              "      <td>former chicagoan appal amalfi hotel chicago fi...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1600 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      deceptive  ...                                            text_pp\n",
              "0             0  ...  stay one night getaway famili thursday tripl a...\n",
              "1             0  ...  tripl rate upgrad view room less also includ b...\n",
              "2             0  ...  come littl late final catch review past sever ...\n",
              "3             0  ...  omni chicago realli deliv front spacious room ...\n",
              "4             0  ...  ask high floor away elev got room pleasantli d...\n",
              "...         ...  ...                                                ...\n",
              "1595          1  ...  problem start book intercontinent chicago onli...\n",
              "1596          1  ...  amalfi hotel beauti websit interior decor wife...\n",
              "1597          1  ...  intercontinent chicago magnific mile outsid ho...\n",
              "1598          1  ...  palmer hous hilton look good pictur outsid act...\n",
              "1599          1  ...  former chicagoan appal amalfi hotel chicago fi...\n",
              "\n",
              "[1600 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihnhEfR9AsD1"
      },
      "source": [
        "# Vectorización de Texto mediante BOW"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "034-OjI58ff2",
        "outputId": "10e157e9-2f5d-4059-eb27-81f5451db27e"
      },
      "source": [
        "vectorizer = TfidfVectorizer(min_df=2)\n",
        "BOW = vectorizer.fit_transform(datos['text_pp'])\n",
        "BOW.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1600, 3616)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "_jB5UK8t_L8V",
        "outputId": "b03dcf26-d97a-4e56-fbe5-b74684d0a205"
      },
      "source": [
        "palabras = vectorizer.get_feature_names()\n",
        "pd.DataFrame(BOW.todense(), index=datos.index, columns=palabras)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>aaa</th>\n",
              "      <th>abil</th>\n",
              "      <th>abl</th>\n",
              "      <th>abrupt</th>\n",
              "      <th>absenc</th>\n",
              "      <th>absent</th>\n",
              "      <th>absolut</th>\n",
              "      <th>absurd</th>\n",
              "      <th>abund</th>\n",
              "      <th>abysm</th>\n",
              "      <th>ac</th>\n",
              "      <th>accent</th>\n",
              "      <th>accept</th>\n",
              "      <th>access</th>\n",
              "      <th>accident</th>\n",
              "      <th>accomad</th>\n",
              "      <th>accomid</th>\n",
              "      <th>accommod</th>\n",
              "      <th>accomod</th>\n",
              "      <th>accompani</th>\n",
              "      <th>accomplish</th>\n",
              "      <th>accord</th>\n",
              "      <th>account</th>\n",
              "      <th>accur</th>\n",
              "      <th>accus</th>\n",
              "      <th>accustom</th>\n",
              "      <th>ach</th>\n",
              "      <th>acknowledg</th>\n",
              "      <th>acquaint</th>\n",
              "      <th>across</th>\n",
              "      <th>act</th>\n",
              "      <th>action</th>\n",
              "      <th>activ</th>\n",
              "      <th>actual</th>\n",
              "      <th>ad</th>\n",
              "      <th>add</th>\n",
              "      <th>addit</th>\n",
              "      <th>address</th>\n",
              "      <th>adequ</th>\n",
              "      <th>adjac</th>\n",
              "      <th>...</th>\n",
              "      <th>worn</th>\n",
              "      <th>worri</th>\n",
              "      <th>wors</th>\n",
              "      <th>worst</th>\n",
              "      <th>worth</th>\n",
              "      <th>worthi</th>\n",
              "      <th>woudl</th>\n",
              "      <th>would</th>\n",
              "      <th>wouldnt</th>\n",
              "      <th>wow</th>\n",
              "      <th>wrap</th>\n",
              "      <th>wrigley</th>\n",
              "      <th>wrinkl</th>\n",
              "      <th>write</th>\n",
              "      <th>written</th>\n",
              "      <th>wrong</th>\n",
              "      <th>wrote</th>\n",
              "      <th>ye</th>\n",
              "      <th>yeah</th>\n",
              "      <th>year</th>\n",
              "      <th>yearli</th>\n",
              "      <th>yell</th>\n",
              "      <th>yellow</th>\n",
              "      <th>yelp</th>\n",
              "      <th>yep</th>\n",
              "      <th>yesterday</th>\n",
              "      <th>yet</th>\n",
              "      <th>yield</th>\n",
              "      <th>yoga</th>\n",
              "      <th>yogurt</th>\n",
              "      <th>york</th>\n",
              "      <th>young</th>\n",
              "      <th>younger</th>\n",
              "      <th>yr</th>\n",
              "      <th>yuck</th>\n",
              "      <th>yummi</th>\n",
              "      <th>yup</th>\n",
              "      <th>zest</th>\n",
              "      <th>zone</th>\n",
              "      <th>zoo</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.1882</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.111565</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.056084</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.304154</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.084973</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.043475</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.181761</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1595</th>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.096682</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1596</th>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.136929</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.128338</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.129032</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1597</th>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.111367</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.154996</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.162156</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1598</th>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.158274</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599</th>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.082425</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1600 rows × 3616 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         aaa  abil       abl  abrupt  absenc  ...  yummi  yup  zest  zone  zoo\n",
              "0     0.1882   0.0  0.000000     0.0     0.0  ...    0.0  0.0   0.0   0.0  0.0\n",
              "1     0.0000   0.0  0.000000     0.0     0.0  ...    0.0  0.0   0.0   0.0  0.0\n",
              "2     0.0000   0.0  0.084973     0.0     0.0  ...    0.0  0.0   0.0   0.0  0.0\n",
              "3     0.0000   0.0  0.000000     0.0     0.0  ...    0.0  0.0   0.0   0.0  0.0\n",
              "4     0.0000   0.0  0.000000     0.0     0.0  ...    0.0  0.0   0.0   0.0  0.0\n",
              "...      ...   ...       ...     ...     ...  ...    ...  ...   ...   ...  ...\n",
              "1595  0.0000   0.0  0.000000     0.0     0.0  ...    0.0  0.0   0.0   0.0  0.0\n",
              "1596  0.0000   0.0  0.000000     0.0     0.0  ...    0.0  0.0   0.0   0.0  0.0\n",
              "1597  0.0000   0.0  0.000000     0.0     0.0  ...    0.0  0.0   0.0   0.0  0.0\n",
              "1598  0.0000   0.0  0.000000     0.0     0.0  ...    0.0  0.0   0.0   0.0  0.0\n",
              "1599  0.0000   0.0  0.000000     0.0     0.0  ...    0.0  0.0   0.0   0.0  0.0\n",
              "\n",
              "[1600 rows x 3616 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcpeYCyCAws2"
      },
      "source": [
        "# Dividir conjunto de entrenamiento y prueba"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhP0EnQSBEv0"
      },
      "source": [
        "X = BOW.todense()\n",
        "y = datos['deceptive']\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5cxknURA7oM"
      },
      "source": [
        "X_train, X_test , y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
        "                                                     random_state=3)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXPtxsn9DDmY",
        "outputId": "3c89fabc-503a-4f4d-aa3e-e5be9b5a7a79"
      },
      "source": [
        "#Conjunto de entrenamiento\n",
        "X_train.shape, y_train.shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1280, 3616), (1280,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9m7lzaudDS4m",
        "outputId": "dd4661a6-90f4-4535-9aae-3398f453d80c"
      },
      "source": [
        "#Conjunto de prueba\n",
        "X_test.shape, y_test.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((320, 3616), (320,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UHJqliwDqm3"
      },
      "source": [
        "# Diseño del modelo de red neuronal usando TensorFlow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjgqDLdOUi1r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "707321a4-da22-4d45-e80e-3483f0a2b1a1"
      },
      "source": [
        "import tensorflow as tf  \n",
        "print(tf.__version__)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPAq3gAuWX3Z"
      },
      "source": [
        "\n",
        "\n",
        "TensorFlow y Keras, ambos proyectos para el aprendizaje profundo,tienen una historia entrelazada. Keras era un conjunto de abstracciones que facilita el aprendizaje profundo, pero necesitada de un backend, desde la versión de Keras v1.1.0 TensorFlow fue el backend predeterminado (antes era Theano).\n",
        "\n",
        "A partir del lanzamiento de TensorFlow a mediados de 2019, Keras es ahora la API de alto nivel de TensorFlow para facilitar el diseño y entrenamiento de modelos rápidos y fáciles.  \n",
        "\n",
        "[Video sobre TensorFlow 2.0](https://www.youtube.com/watch?v=EqWsPO8DVXk)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKxI5MTNQ29i",
        "outputId": "370c16a5-ab44-465b-d4e8-1f3ef1051ad9"
      },
      "source": [
        "model = tf.keras.Sequential([                     # https://www.tensorflow.org/guide/keras/sequential_model\n",
        "                             \n",
        "        tf.keras.Input(shape=(3616,)),              # dimensiones de entrada\n",
        "    \n",
        "        tf.keras.layers.Dense( \n",
        "                              1,                     # dimensiones de salida\n",
        "                              activation='sigmoid',  # función de activación  https://www.tensorflow.org/api_docs/python/tf/keras/activations\n",
        "                              name=\"layer_1\"         # nombre de la capa\n",
        "\n",
        "                              )\n",
        "])\n",
        "model.summary()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "layer_1 (Dense)              (None, 1)                 3617      \n",
            "=================================================================\n",
            "Total params: 3,617\n",
            "Trainable params: 3,617\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKD8TCnERdjN"
      },
      "source": [
        "Una alternativa para establecer las dimensiones de entrada en la primera capa es utilizar el parámetro `input_shape`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSAJv-bJUoNn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2a5f883-c8a3-4b5d-b0ab-93413cf3c9bf"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "    \n",
        "          tf.keras.layers.Dense(1, \n",
        "                                input_shape = (3616,),       # dimensiones de la entrada  \n",
        "                                activation='sigmoid',        # para la regresión logística\n",
        "                                name=\"layer_1\"               # nombre de la capa\n",
        "                                ),\n",
        "])\n",
        "model.summary()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "layer_1 (Dense)              (None, 1)                 3617      \n",
            "=================================================================\n",
            "Total params: 3,617\n",
            "Trainable params: 3,617\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHjpA_I3P92N"
      },
      "source": [
        "Otra manera de construir un modelo secuencial es declrarlo y a continuación añadir capas ustilizando el método `add`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75thWRxDPzBh",
        "outputId": "de46ce34-b193-49c6-8398-c9df780be76b"
      },
      "source": [
        "model = tf.keras.Sequential()\n",
        "    \n",
        "model.add(tf.keras.Input(shape=(3616,)))\n",
        "\n",
        "model.add(tf.keras.layers.Dense(1,\n",
        "                                  activation='sigmoid',        # para la regresión logística\n",
        "                                  name=\"layer_1\"   \n",
        "                                 ))\n",
        "model.summary()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "layer_1 (Dense)              (None, 1)                 3617      \n",
            "=================================================================\n",
            "Total params: 3,617\n",
            "Trainable params: 3,617\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAxKjodjP6Kk",
        "outputId": "349069e2-811b-4793-fa2e-2860fe56d432"
      },
      "source": [
        "model.add(tf.keras.layers.Dense(1,\n",
        "                                  activation='sigmoid',       \n",
        "                                  name=\"layer_2\")  \n",
        "                                  )\n",
        "model.summary()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "layer_1 (Dense)              (None, 1)                 3617      \n",
            "_________________________________________________________________\n",
            "layer_2 (Dense)              (None, 1)                 2         \n",
            "=================================================================\n",
            "Total params: 3,619\n",
            "Trainable params: 3,619\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcvcyUIKPYdl",
        "outputId": "fc5d4873-81c1-4511-9f01-1beb53470b35"
      },
      "source": [
        "len(model.layers)  # layers es un atributo del modelo que regresa una lista con las capas del modelo"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "uCQHIuKbRp_r",
        "outputId": "bb4e6099-558d-431c-8eac-0224987517a6"
      },
      "source": [
        "tf.keras.utils.plot_model( \n",
        "    model,\n",
        "    to_file=\"model.png\",\n",
        "    show_shapes=True,\n",
        "    show_dtype=False,\n",
        "    show_layer_names=True,\n",
        "    rankdir=\"LR\",\n",
        "    dpi=96,\n",
        ")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJS9U_j2PVeU",
        "outputId": "8de19dfb-e38e-41e2-d04b-e04c7de9b25c"
      },
      "source": [
        "\n",
        "model.pop()    # borra la última capa\n",
        "print(len(model.layers))\n",
        "model.summary()\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "layer_1 (Dense)              (None, 1)                 3617      \n",
            "=================================================================\n",
            "Total params: 3,617\n",
            "Trainable params: 3,617\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEj4kqqjNfRW"
      },
      "source": [
        "# Entrenamiento y validación\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUXjspeTIoEE"
      },
      "source": [
        "model.compile(optimizer='adam',             # stochastic gradient descent adaptativo\n",
        "                                            # https://keras.io/api/optimizers/adam/\n",
        "              \n",
        "              loss='binary_crossentropy',   # función objetivo  que se busca minimizar\n",
        "                                            # https://keras.io/api/losses/\n",
        "              \n",
        "              metrics=['accuracy']\n",
        "              )\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9gjfbUKItmk",
        "outputId": "bf58e756-b1a9-48b6-f016-856e3a35e5c4"
      },
      "source": [
        "model.fit(X_train, y_train, epochs=100, batch_size=100, validation_data=(X_test, y_test))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "13/13 [==============================] - 0s 13ms/step - loss: 0.6917 - accuracy: 0.5531 - val_loss: 0.6903 - val_accuracy: 0.5750\n",
            "Epoch 2/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.6850 - accuracy: 0.7195 - val_loss: 0.6872 - val_accuracy: 0.6438\n",
            "Epoch 3/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.6789 - accuracy: 0.7891 - val_loss: 0.6842 - val_accuracy: 0.6438\n",
            "Epoch 4/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.6729 - accuracy: 0.8125 - val_loss: 0.6812 - val_accuracy: 0.6406\n",
            "Epoch 5/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.6671 - accuracy: 0.8297 - val_loss: 0.6782 - val_accuracy: 0.6719\n",
            "Epoch 6/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.6613 - accuracy: 0.8367 - val_loss: 0.6754 - val_accuracy: 0.6781\n",
            "Epoch 7/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.6557 - accuracy: 0.8461 - val_loss: 0.6724 - val_accuracy: 0.6906\n",
            "Epoch 8/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.6501 - accuracy: 0.8625 - val_loss: 0.6691 - val_accuracy: 0.7156\n",
            "Epoch 9/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.6446 - accuracy: 0.8687 - val_loss: 0.6663 - val_accuracy: 0.7156\n",
            "Epoch 10/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.6392 - accuracy: 0.8781 - val_loss: 0.6631 - val_accuracy: 0.7219\n",
            "Epoch 11/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.6338 - accuracy: 0.8852 - val_loss: 0.6601 - val_accuracy: 0.7344\n",
            "Epoch 12/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.6285 - accuracy: 0.8953 - val_loss: 0.6568 - val_accuracy: 0.7437\n",
            "Epoch 13/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.6234 - accuracy: 0.8969 - val_loss: 0.6542 - val_accuracy: 0.7437\n",
            "Epoch 14/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.6181 - accuracy: 0.8984 - val_loss: 0.6512 - val_accuracy: 0.7500\n",
            "Epoch 15/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.6131 - accuracy: 0.9047 - val_loss: 0.6481 - val_accuracy: 0.7563\n",
            "Epoch 16/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.6080 - accuracy: 0.9133 - val_loss: 0.6447 - val_accuracy: 0.7719\n",
            "Epoch 17/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.6031 - accuracy: 0.9227 - val_loss: 0.6417 - val_accuracy: 0.7750\n",
            "Epoch 18/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5982 - accuracy: 0.9250 - val_loss: 0.6390 - val_accuracy: 0.7750\n",
            "Epoch 19/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5933 - accuracy: 0.9312 - val_loss: 0.6359 - val_accuracy: 0.7844\n",
            "Epoch 20/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.5886 - accuracy: 0.9328 - val_loss: 0.6332 - val_accuracy: 0.7875\n",
            "Epoch 21/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5838 - accuracy: 0.9352 - val_loss: 0.6302 - val_accuracy: 0.7906\n",
            "Epoch 22/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.5792 - accuracy: 0.9359 - val_loss: 0.6275 - val_accuracy: 0.7906\n",
            "Epoch 23/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.5745 - accuracy: 0.9375 - val_loss: 0.6248 - val_accuracy: 0.7969\n",
            "Epoch 24/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.5700 - accuracy: 0.9383 - val_loss: 0.6220 - val_accuracy: 0.8031\n",
            "Epoch 25/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.5655 - accuracy: 0.9414 - val_loss: 0.6191 - val_accuracy: 0.8094\n",
            "Epoch 26/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.5611 - accuracy: 0.9422 - val_loss: 0.6164 - val_accuracy: 0.8156\n",
            "Epoch 27/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.5567 - accuracy: 0.9438 - val_loss: 0.6137 - val_accuracy: 0.8156\n",
            "Epoch 28/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5524 - accuracy: 0.9445 - val_loss: 0.6113 - val_accuracy: 0.8156\n",
            "Epoch 29/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.5481 - accuracy: 0.9477 - val_loss: 0.6083 - val_accuracy: 0.8125\n",
            "Epoch 30/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.5439 - accuracy: 0.9477 - val_loss: 0.6059 - val_accuracy: 0.8125\n",
            "Epoch 31/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.5397 - accuracy: 0.9492 - val_loss: 0.6034 - val_accuracy: 0.8125\n",
            "Epoch 32/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5356 - accuracy: 0.9492 - val_loss: 0.6010 - val_accuracy: 0.8125\n",
            "Epoch 33/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5316 - accuracy: 0.9500 - val_loss: 0.5986 - val_accuracy: 0.8125\n",
            "Epoch 34/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.5276 - accuracy: 0.9508 - val_loss: 0.5958 - val_accuracy: 0.8188\n",
            "Epoch 35/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.5237 - accuracy: 0.9508 - val_loss: 0.5938 - val_accuracy: 0.8125\n",
            "Epoch 36/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.5197 - accuracy: 0.9516 - val_loss: 0.5913 - val_accuracy: 0.8188\n",
            "Epoch 37/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.5159 - accuracy: 0.9523 - val_loss: 0.5890 - val_accuracy: 0.8188\n",
            "Epoch 38/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5120 - accuracy: 0.9523 - val_loss: 0.5865 - val_accuracy: 0.8219\n",
            "Epoch 39/100\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5083 - accuracy: 0.9523 - val_loss: 0.5841 - val_accuracy: 0.8250\n",
            "Epoch 40/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5046 - accuracy: 0.9523 - val_loss: 0.5819 - val_accuracy: 0.8250\n",
            "Epoch 41/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5009 - accuracy: 0.9531 - val_loss: 0.5798 - val_accuracy: 0.8250\n",
            "Epoch 42/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.4972 - accuracy: 0.9531 - val_loss: 0.5774 - val_accuracy: 0.8250\n",
            "Epoch 43/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.4937 - accuracy: 0.9539 - val_loss: 0.5750 - val_accuracy: 0.8281\n",
            "Epoch 44/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.4901 - accuracy: 0.9555 - val_loss: 0.5731 - val_accuracy: 0.8281\n",
            "Epoch 45/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.4866 - accuracy: 0.9555 - val_loss: 0.5711 - val_accuracy: 0.8281\n",
            "Epoch 46/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.4831 - accuracy: 0.9563 - val_loss: 0.5689 - val_accuracy: 0.8313\n",
            "Epoch 47/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.4797 - accuracy: 0.9555 - val_loss: 0.5667 - val_accuracy: 0.8313\n",
            "Epoch 48/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.4763 - accuracy: 0.9555 - val_loss: 0.5646 - val_accuracy: 0.8313\n",
            "Epoch 49/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.4730 - accuracy: 0.9547 - val_loss: 0.5626 - val_accuracy: 0.8313\n",
            "Epoch 50/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.4697 - accuracy: 0.9563 - val_loss: 0.5606 - val_accuracy: 0.8281\n",
            "Epoch 51/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.4664 - accuracy: 0.9563 - val_loss: 0.5585 - val_accuracy: 0.8281\n",
            "Epoch 52/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.4631 - accuracy: 0.9570 - val_loss: 0.5566 - val_accuracy: 0.8281\n",
            "Epoch 53/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.4600 - accuracy: 0.9578 - val_loss: 0.5546 - val_accuracy: 0.8281\n",
            "Epoch 54/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.4568 - accuracy: 0.9594 - val_loss: 0.5527 - val_accuracy: 0.8281\n",
            "Epoch 55/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.4537 - accuracy: 0.9586 - val_loss: 0.5510 - val_accuracy: 0.8281\n",
            "Epoch 56/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.4506 - accuracy: 0.9594 - val_loss: 0.5491 - val_accuracy: 0.8281\n",
            "Epoch 57/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.4476 - accuracy: 0.9609 - val_loss: 0.5470 - val_accuracy: 0.8313\n",
            "Epoch 58/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.4445 - accuracy: 0.9609 - val_loss: 0.5452 - val_accuracy: 0.8344\n",
            "Epoch 59/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.4415 - accuracy: 0.9609 - val_loss: 0.5433 - val_accuracy: 0.8375\n",
            "Epoch 60/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.4386 - accuracy: 0.9602 - val_loss: 0.5414 - val_accuracy: 0.8406\n",
            "Epoch 61/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.4357 - accuracy: 0.9617 - val_loss: 0.5398 - val_accuracy: 0.8438\n",
            "Epoch 62/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.4328 - accuracy: 0.9617 - val_loss: 0.5382 - val_accuracy: 0.8344\n",
            "Epoch 63/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.4299 - accuracy: 0.9617 - val_loss: 0.5364 - val_accuracy: 0.8438\n",
            "Epoch 64/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.4271 - accuracy: 0.9633 - val_loss: 0.5345 - val_accuracy: 0.8406\n",
            "Epoch 65/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.4243 - accuracy: 0.9633 - val_loss: 0.5329 - val_accuracy: 0.8438\n",
            "Epoch 66/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.4215 - accuracy: 0.9641 - val_loss: 0.5312 - val_accuracy: 0.8406\n",
            "Epoch 67/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.4188 - accuracy: 0.9648 - val_loss: 0.5294 - val_accuracy: 0.8438\n",
            "Epoch 68/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.4161 - accuracy: 0.9648 - val_loss: 0.5279 - val_accuracy: 0.8438\n",
            "Epoch 69/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.4134 - accuracy: 0.9648 - val_loss: 0.5263 - val_accuracy: 0.8438\n",
            "Epoch 70/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.4108 - accuracy: 0.9648 - val_loss: 0.5247 - val_accuracy: 0.8438\n",
            "Epoch 71/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.4082 - accuracy: 0.9664 - val_loss: 0.5230 - val_accuracy: 0.8438\n",
            "Epoch 72/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.4056 - accuracy: 0.9664 - val_loss: 0.5215 - val_accuracy: 0.8438\n",
            "Epoch 73/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.4030 - accuracy: 0.9664 - val_loss: 0.5200 - val_accuracy: 0.8438\n",
            "Epoch 74/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.4005 - accuracy: 0.9672 - val_loss: 0.5184 - val_accuracy: 0.8438\n",
            "Epoch 75/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.3980 - accuracy: 0.9672 - val_loss: 0.5169 - val_accuracy: 0.8438\n",
            "Epoch 76/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.3955 - accuracy: 0.9672 - val_loss: 0.5154 - val_accuracy: 0.8438\n",
            "Epoch 77/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.3930 - accuracy: 0.9672 - val_loss: 0.5139 - val_accuracy: 0.8406\n",
            "Epoch 78/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.3906 - accuracy: 0.9680 - val_loss: 0.5122 - val_accuracy: 0.8469\n",
            "Epoch 79/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.3882 - accuracy: 0.9672 - val_loss: 0.5109 - val_accuracy: 0.8469\n",
            "Epoch 80/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.3858 - accuracy: 0.9680 - val_loss: 0.5095 - val_accuracy: 0.8469\n",
            "Epoch 81/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.3835 - accuracy: 0.9680 - val_loss: 0.5082 - val_accuracy: 0.8469\n",
            "Epoch 82/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.3811 - accuracy: 0.9680 - val_loss: 0.5068 - val_accuracy: 0.8469\n",
            "Epoch 83/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.3788 - accuracy: 0.9680 - val_loss: 0.5054 - val_accuracy: 0.8469\n",
            "Epoch 84/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.3765 - accuracy: 0.9703 - val_loss: 0.5041 - val_accuracy: 0.8469\n",
            "Epoch 85/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.3743 - accuracy: 0.9703 - val_loss: 0.5026 - val_accuracy: 0.8469\n",
            "Epoch 86/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.3720 - accuracy: 0.9703 - val_loss: 0.5014 - val_accuracy: 0.8469\n",
            "Epoch 87/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.3698 - accuracy: 0.9703 - val_loss: 0.5000 - val_accuracy: 0.8469\n",
            "Epoch 88/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.3676 - accuracy: 0.9703 - val_loss: 0.4987 - val_accuracy: 0.8469\n",
            "Epoch 89/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.3654 - accuracy: 0.9703 - val_loss: 0.4971 - val_accuracy: 0.8469\n",
            "Epoch 90/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.3633 - accuracy: 0.9703 - val_loss: 0.4960 - val_accuracy: 0.8469\n",
            "Epoch 91/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.3611 - accuracy: 0.9703 - val_loss: 0.4948 - val_accuracy: 0.8469\n",
            "Epoch 92/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.3590 - accuracy: 0.9711 - val_loss: 0.4934 - val_accuracy: 0.8469\n",
            "Epoch 93/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.3569 - accuracy: 0.9711 - val_loss: 0.4922 - val_accuracy: 0.8469\n",
            "Epoch 94/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.3548 - accuracy: 0.9711 - val_loss: 0.4909 - val_accuracy: 0.8469\n",
            "Epoch 95/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.3528 - accuracy: 0.9711 - val_loss: 0.4896 - val_accuracy: 0.8469\n",
            "Epoch 96/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.3508 - accuracy: 0.9711 - val_loss: 0.4884 - val_accuracy: 0.8469\n",
            "Epoch 97/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.3488 - accuracy: 0.9711 - val_loss: 0.4872 - val_accuracy: 0.8469\n",
            "Epoch 98/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.3467 - accuracy: 0.9719 - val_loss: 0.4860 - val_accuracy: 0.8469\n",
            "Epoch 99/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.3448 - accuracy: 0.9719 - val_loss: 0.4850 - val_accuracy: 0.8469\n",
            "Epoch 100/100\n",
            "13/13 [==============================] - 0s 3ms/step - loss: 0.3428 - accuracy: 0.9727 - val_loss: 0.4839 - val_accuracy: 0.8469\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f66d2c1cc90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgfMP_nFUQs8"
      },
      "source": [
        "# Red neuronal de clasificación con más de una capa"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "keYY20FtbWiy",
        "outputId": "2bf8c82f-d821-4c19-d555-d8fbc9d8780b"
      },
      "source": [
        "model = tf.keras.Sequential()\n",
        "    \n",
        "model.add((tf.keras.layers.InputLayer(input_shape=(3616,))))\n",
        "\n",
        "model.add(tf.keras.layers.Dense(12, activation='sigmoid',name=\"hidden_layer_1\" ))\n",
        "\n",
        "model.add(tf.keras.layers.Dense(12, activation='sigmoid', name=\"hidden_layer_2\"))\n",
        "\n",
        "model.add(tf.keras.layers.Dense(1,activation='sigmoid', name=\"output_layer\" ))\n",
        "\n",
        "\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "hidden_layer_1 (Dense)       (None, 12)                43404     \n",
            "_________________________________________________________________\n",
            "hidden_layer_2 (Dense)       (None, 12)                156       \n",
            "_________________________________________________________________\n",
            "output_layer (Dense)         (None, 1)                 13        \n",
            "=================================================================\n",
            "Total params: 43,573\n",
            "Trainable params: 43,573\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "_gWCVSisk_cK",
        "outputId": "c0a94ade-d037-4767-e5c0-d65fc957afc0"
      },
      "source": [
        "tf.keras.utils.plot_model( \n",
        "    model,\n",
        "    to_file=\"model.png\",\n",
        "    show_shapes=True,\n",
        "    show_dtype=False,\n",
        "    show_layer_names=True,\n",
        "    rankdir=\"LR\",\n",
        "    dpi=96,\n",
        ")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDdoPn6he4P-",
        "outputId": "9e812089-85d7-43a6-a5f4-cc10217dc265"
      },
      "source": [
        "model.compile(optimizer='adam',             # stochastic gradient descent adaptativo\n",
        "                                            # https://keras.io/api/optimizers/adam/\n",
        "              \n",
        "              loss='binary_crossentropy',   # función objetivo  que se busca minimizar\n",
        "                                            # https://keras.io/api/losses/\n",
        "              \n",
        "              metrics=['accuracy']\n",
        "              )\n",
        "model.fit(X_train, y_train, epochs=100, batch_size=100, validation_data=(X_test, y_test))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "13/13 [==============================] - 1s 14ms/step - loss: 0.6970 - accuracy: 0.5156 - val_loss: 0.7088 - val_accuracy: 0.4375\n",
            "Epoch 2/100\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.6922 - accuracy: 0.5156 - val_loss: 0.6998 - val_accuracy: 0.4375\n",
            "Epoch 3/100\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.6899 - accuracy: 0.5156 - val_loss: 0.6956 - val_accuracy: 0.4375\n",
            "Epoch 4/100\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.6886 - accuracy: 0.5156 - val_loss: 0.6937 - val_accuracy: 0.4375\n",
            "Epoch 5/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.6869 - accuracy: 0.5156 - val_loss: 0.6937 - val_accuracy: 0.4375\n",
            "Epoch 6/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.6854 - accuracy: 0.5156 - val_loss: 0.6920 - val_accuracy: 0.4375\n",
            "Epoch 7/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.6835 - accuracy: 0.5156 - val_loss: 0.6917 - val_accuracy: 0.4375\n",
            "Epoch 8/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.6813 - accuracy: 0.5172 - val_loss: 0.6898 - val_accuracy: 0.4406\n",
            "Epoch 9/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.6791 - accuracy: 0.5188 - val_loss: 0.6898 - val_accuracy: 0.4375\n",
            "Epoch 10/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.6763 - accuracy: 0.5266 - val_loss: 0.6873 - val_accuracy: 0.4531\n",
            "Epoch 11/100\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.6731 - accuracy: 0.6172 - val_loss: 0.6837 - val_accuracy: 0.5094\n",
            "Epoch 12/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.6692 - accuracy: 0.6562 - val_loss: 0.6824 - val_accuracy: 0.4875\n",
            "Epoch 13/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.6649 - accuracy: 0.6352 - val_loss: 0.6799 - val_accuracy: 0.5094\n",
            "Epoch 14/100\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.6596 - accuracy: 0.7117 - val_loss: 0.6758 - val_accuracy: 0.6000\n",
            "Epoch 15/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.6538 - accuracy: 0.7773 - val_loss: 0.6730 - val_accuracy: 0.5750\n",
            "Epoch 16/100\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.6465 - accuracy: 0.8305 - val_loss: 0.6667 - val_accuracy: 0.6938\n",
            "Epoch 17/100\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.6386 - accuracy: 0.8930 - val_loss: 0.6612 - val_accuracy: 0.7406\n",
            "Epoch 18/100\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.6291 - accuracy: 0.9000 - val_loss: 0.6554 - val_accuracy: 0.7469\n",
            "Epoch 19/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.6187 - accuracy: 0.8789 - val_loss: 0.6507 - val_accuracy: 0.7250\n",
            "Epoch 20/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.6067 - accuracy: 0.9008 - val_loss: 0.6414 - val_accuracy: 0.7594\n",
            "Epoch 21/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5931 - accuracy: 0.9258 - val_loss: 0.6329 - val_accuracy: 0.7688\n",
            "Epoch 22/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5784 - accuracy: 0.9297 - val_loss: 0.6232 - val_accuracy: 0.7875\n",
            "Epoch 23/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5623 - accuracy: 0.9375 - val_loss: 0.6127 - val_accuracy: 0.8094\n",
            "Epoch 24/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5448 - accuracy: 0.9430 - val_loss: 0.6016 - val_accuracy: 0.8250\n",
            "Epoch 25/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5262 - accuracy: 0.9453 - val_loss: 0.5894 - val_accuracy: 0.8250\n",
            "Epoch 26/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.5063 - accuracy: 0.9523 - val_loss: 0.5764 - val_accuracy: 0.8281\n",
            "Epoch 27/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.4855 - accuracy: 0.9547 - val_loss: 0.5633 - val_accuracy: 0.8344\n",
            "Epoch 28/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.4638 - accuracy: 0.9570 - val_loss: 0.5491 - val_accuracy: 0.8406\n",
            "Epoch 29/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.4418 - accuracy: 0.9586 - val_loss: 0.5344 - val_accuracy: 0.8313\n",
            "Epoch 30/100\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.4197 - accuracy: 0.9586 - val_loss: 0.5212 - val_accuracy: 0.8375\n",
            "Epoch 31/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.3975 - accuracy: 0.9609 - val_loss: 0.5073 - val_accuracy: 0.8375\n",
            "Epoch 32/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.3760 - accuracy: 0.9617 - val_loss: 0.4939 - val_accuracy: 0.8375\n",
            "Epoch 33/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.3550 - accuracy: 0.9648 - val_loss: 0.4801 - val_accuracy: 0.8438\n",
            "Epoch 34/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.3345 - accuracy: 0.9664 - val_loss: 0.4689 - val_accuracy: 0.8438\n",
            "Epoch 35/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.3155 - accuracy: 0.9664 - val_loss: 0.4581 - val_accuracy: 0.8438\n",
            "Epoch 36/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.2970 - accuracy: 0.9711 - val_loss: 0.4460 - val_accuracy: 0.8438\n",
            "Epoch 37/100\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.2796 - accuracy: 0.9719 - val_loss: 0.4347 - val_accuracy: 0.8469\n",
            "Epoch 38/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.2634 - accuracy: 0.9727 - val_loss: 0.4256 - val_accuracy: 0.8469\n",
            "Epoch 39/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.2481 - accuracy: 0.9742 - val_loss: 0.4173 - val_accuracy: 0.8500\n",
            "Epoch 40/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.2339 - accuracy: 0.9766 - val_loss: 0.4091 - val_accuracy: 0.8500\n",
            "Epoch 41/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.2206 - accuracy: 0.9789 - val_loss: 0.4026 - val_accuracy: 0.8531\n",
            "Epoch 42/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.2081 - accuracy: 0.9797 - val_loss: 0.3956 - val_accuracy: 0.8531\n",
            "Epoch 43/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.1964 - accuracy: 0.9805 - val_loss: 0.3896 - val_accuracy: 0.8531\n",
            "Epoch 44/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.1856 - accuracy: 0.9828 - val_loss: 0.3844 - val_accuracy: 0.8531\n",
            "Epoch 45/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.1756 - accuracy: 0.9828 - val_loss: 0.3806 - val_accuracy: 0.8531\n",
            "Epoch 46/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.1661 - accuracy: 0.9836 - val_loss: 0.3751 - val_accuracy: 0.8531\n",
            "Epoch 47/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.1573 - accuracy: 0.9844 - val_loss: 0.3713 - val_accuracy: 0.8531\n",
            "Epoch 48/100\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.1489 - accuracy: 0.9844 - val_loss: 0.3677 - val_accuracy: 0.8531\n",
            "Epoch 49/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.1412 - accuracy: 0.9852 - val_loss: 0.3643 - val_accuracy: 0.8500\n",
            "Epoch 50/100\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.1341 - accuracy: 0.9875 - val_loss: 0.3605 - val_accuracy: 0.8500\n",
            "Epoch 51/100\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.1272 - accuracy: 0.9891 - val_loss: 0.3599 - val_accuracy: 0.8438\n",
            "Epoch 52/100\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.1208 - accuracy: 0.9906 - val_loss: 0.3574 - val_accuracy: 0.8438\n",
            "Epoch 53/100\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.1148 - accuracy: 0.9906 - val_loss: 0.3546 - val_accuracy: 0.8500\n",
            "Epoch 54/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.1091 - accuracy: 0.9906 - val_loss: 0.3529 - val_accuracy: 0.8469\n",
            "Epoch 55/100\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.1039 - accuracy: 0.9914 - val_loss: 0.3522 - val_accuracy: 0.8438\n",
            "Epoch 56/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.0988 - accuracy: 0.9914 - val_loss: 0.3506 - val_accuracy: 0.8438\n",
            "Epoch 57/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.0941 - accuracy: 0.9922 - val_loss: 0.3479 - val_accuracy: 0.8438\n",
            "Epoch 58/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.0897 - accuracy: 0.9937 - val_loss: 0.3470 - val_accuracy: 0.8438\n",
            "Epoch 59/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.0855 - accuracy: 0.9945 - val_loss: 0.3481 - val_accuracy: 0.8469\n",
            "Epoch 60/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.0816 - accuracy: 0.9953 - val_loss: 0.3468 - val_accuracy: 0.8469\n",
            "Epoch 61/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.0777 - accuracy: 0.9961 - val_loss: 0.3469 - val_accuracy: 0.8469\n",
            "Epoch 62/100\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.0743 - accuracy: 0.9961 - val_loss: 0.3460 - val_accuracy: 0.8406\n",
            "Epoch 63/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.0709 - accuracy: 0.9961 - val_loss: 0.3455 - val_accuracy: 0.8406\n",
            "Epoch 64/100\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.0678 - accuracy: 0.9969 - val_loss: 0.3458 - val_accuracy: 0.8469\n",
            "Epoch 65/100\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.0649 - accuracy: 0.9977 - val_loss: 0.3458 - val_accuracy: 0.8469\n",
            "Epoch 66/100\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.0620 - accuracy: 0.9984 - val_loss: 0.3447 - val_accuracy: 0.8406\n",
            "Epoch 67/100\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.0594 - accuracy: 0.9984 - val_loss: 0.3468 - val_accuracy: 0.8469\n",
            "Epoch 68/100\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.0568 - accuracy: 0.9992 - val_loss: 0.3468 - val_accuracy: 0.8438\n",
            "Epoch 69/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.0544 - accuracy: 0.9992 - val_loss: 0.3456 - val_accuracy: 0.8438\n",
            "Epoch 70/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.0522 - accuracy: 0.9992 - val_loss: 0.3462 - val_accuracy: 0.8438\n",
            "Epoch 71/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.0500 - accuracy: 0.9992 - val_loss: 0.3470 - val_accuracy: 0.8438\n",
            "Epoch 72/100\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.0480 - accuracy: 0.9992 - val_loss: 0.3476 - val_accuracy: 0.8406\n",
            "Epoch 73/100\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.0461 - accuracy: 1.0000 - val_loss: 0.3481 - val_accuracy: 0.8406\n",
            "Epoch 74/100\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.0443 - accuracy: 1.0000 - val_loss: 0.3484 - val_accuracy: 0.8438\n",
            "Epoch 75/100\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.0426 - accuracy: 1.0000 - val_loss: 0.3487 - val_accuracy: 0.8406\n",
            "Epoch 76/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.0409 - accuracy: 1.0000 - val_loss: 0.3494 - val_accuracy: 0.8406\n",
            "Epoch 77/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.0394 - accuracy: 1.0000 - val_loss: 0.3500 - val_accuracy: 0.8406\n",
            "Epoch 78/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.0379 - accuracy: 1.0000 - val_loss: 0.3511 - val_accuracy: 0.8438\n",
            "Epoch 79/100\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.0366 - accuracy: 1.0000 - val_loss: 0.3516 - val_accuracy: 0.8406\n",
            "Epoch 80/100\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.0352 - accuracy: 1.0000 - val_loss: 0.3523 - val_accuracy: 0.8406\n",
            "Epoch 81/100\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.0340 - accuracy: 1.0000 - val_loss: 0.3531 - val_accuracy: 0.8406\n",
            "Epoch 82/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.0328 - accuracy: 1.0000 - val_loss: 0.3534 - val_accuracy: 0.8406\n",
            "Epoch 83/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.0316 - accuracy: 1.0000 - val_loss: 0.3547 - val_accuracy: 0.8438\n",
            "Epoch 84/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.0306 - accuracy: 1.0000 - val_loss: 0.3560 - val_accuracy: 0.8438\n",
            "Epoch 85/100\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.0295 - accuracy: 1.0000 - val_loss: 0.3563 - val_accuracy: 0.8438\n",
            "Epoch 86/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.0285 - accuracy: 1.0000 - val_loss: 0.3566 - val_accuracy: 0.8406\n",
            "Epoch 87/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.0276 - accuracy: 1.0000 - val_loss: 0.3581 - val_accuracy: 0.8438\n",
            "Epoch 88/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.0267 - accuracy: 1.0000 - val_loss: 0.3583 - val_accuracy: 0.8406\n",
            "Epoch 89/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.0258 - accuracy: 1.0000 - val_loss: 0.3599 - val_accuracy: 0.8438\n",
            "Epoch 90/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.0250 - accuracy: 1.0000 - val_loss: 0.3609 - val_accuracy: 0.8438\n",
            "Epoch 91/100\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.0243 - accuracy: 1.0000 - val_loss: 0.3615 - val_accuracy: 0.8438\n",
            "Epoch 92/100\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.0235 - accuracy: 1.0000 - val_loss: 0.3627 - val_accuracy: 0.8438\n",
            "Epoch 93/100\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.0228 - accuracy: 1.0000 - val_loss: 0.3635 - val_accuracy: 0.8438\n",
            "Epoch 94/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.0221 - accuracy: 1.0000 - val_loss: 0.3640 - val_accuracy: 0.8438\n",
            "Epoch 95/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.0215 - accuracy: 1.0000 - val_loss: 0.3652 - val_accuracy: 0.8438\n",
            "Epoch 96/100\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.0209 - accuracy: 1.0000 - val_loss: 0.3665 - val_accuracy: 0.8438\n",
            "Epoch 97/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.0203 - accuracy: 1.0000 - val_loss: 0.3670 - val_accuracy: 0.8438\n",
            "Epoch 98/100\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.0197 - accuracy: 1.0000 - val_loss: 0.3679 - val_accuracy: 0.8438\n",
            "Epoch 99/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.0191 - accuracy: 1.0000 - val_loss: 0.3691 - val_accuracy: 0.8438\n",
            "Epoch 100/100\n",
            "13/13 [==============================] - 0s 4ms/step - loss: 0.0186 - accuracy: 1.0000 - val_loss: 0.3698 - val_accuracy: 0.8438\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f66cd149710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0RxZCrHva61q"
      },
      "source": [
        "# Referencia:\n",
        "\n",
        "1. Keras vs. tf.keras: What’s the difference in TensorFlow 2.0? [Internet]. PyImageSearch. 2019 [citado 20 de julio de 2021]. Disponible en: https://www.pyimagesearch.com/2019/10/21/keras-vs-tf-keras-whats-the-difference-in-tensorflow-2-0/\n"
      ]
    }
  ]
}