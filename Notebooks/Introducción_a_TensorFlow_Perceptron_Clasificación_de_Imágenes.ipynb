{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Introducción a TensorFlow_Perceptron Clasificación de Imágenes.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnIsAsPe/RNN-Resumenes-de-Texto/blob/main/Notebooks/Introducci%C3%B3n_a_TensorFlow_Perceptron_Clasificaci%C3%B3n_de_Im%C3%A1genes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec2m7RHibrJz"
      },
      "source": [
        "# Lectura de bibliotecas y Datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0trJmd6DjqBZ"
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2dmuKS83QCC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "outputId": "edcd737e-02fd-4262-a557-e4a11848813a"
      },
      "source": [
        "datos = pd.read_csv('/content/drive/MyDrive/Datos/img_cancer_26x26pixeles_con_etiqueta.csv')\n",
        "\n",
        "print(datos.shape)\n",
        "datos.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5063, 677)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>637</th>\n",
              "      <th>638</th>\n",
              "      <th>639</th>\n",
              "      <th>640</th>\n",
              "      <th>641</th>\n",
              "      <th>642</th>\n",
              "      <th>643</th>\n",
              "      <th>644</th>\n",
              "      <th>645</th>\n",
              "      <th>646</th>\n",
              "      <th>647</th>\n",
              "      <th>648</th>\n",
              "      <th>649</th>\n",
              "      <th>650</th>\n",
              "      <th>651</th>\n",
              "      <th>652</th>\n",
              "      <th>653</th>\n",
              "      <th>654</th>\n",
              "      <th>655</th>\n",
              "      <th>656</th>\n",
              "      <th>657</th>\n",
              "      <th>658</th>\n",
              "      <th>659</th>\n",
              "      <th>660</th>\n",
              "      <th>661</th>\n",
              "      <th>662</th>\n",
              "      <th>663</th>\n",
              "      <th>664</th>\n",
              "      <th>665</th>\n",
              "      <th>666</th>\n",
              "      <th>667</th>\n",
              "      <th>668</th>\n",
              "      <th>669</th>\n",
              "      <th>670</th>\n",
              "      <th>671</th>\n",
              "      <th>672</th>\n",
              "      <th>673</th>\n",
              "      <th>674</th>\n",
              "      <th>675</th>\n",
              "      <th>clase</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083225</td>\n",
              "      <td>0.083405</td>\n",
              "      <td>0.086613</td>\n",
              "      <td>0.094121</td>\n",
              "      <td>0.095760</td>\n",
              "      <td>0.095939</td>\n",
              "      <td>0.095679</td>\n",
              "      <td>0.095800</td>\n",
              "      <td>0.096059</td>\n",
              "      <td>0.095816</td>\n",
              "      <td>0.097095</td>\n",
              "      <td>0.099579</td>\n",
              "      <td>0.104371</td>\n",
              "      <td>0.121509</td>\n",
              "      <td>0.151148</td>\n",
              "      <td>0.166485</td>\n",
              "      <td>0.142229</td>\n",
              "      <td>0.100965</td>\n",
              "      <td>0.084538</td>\n",
              "      <td>0.083237</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083240</td>\n",
              "      <td>0.084179</td>\n",
              "      <td>0.092702</td>\n",
              "      <td>0.113767</td>\n",
              "      <td>0.125615</td>\n",
              "      <td>0.128518</td>\n",
              "      <td>0.128971</td>\n",
              "      <td>0.129441</td>\n",
              "      <td>0.130263</td>\n",
              "      <td>0.130819</td>\n",
              "      <td>0.130473</td>\n",
              "      <td>0.130028</td>\n",
              "      <td>...</td>\n",
              "      <td>0.188656</td>\n",
              "      <td>0.188316</td>\n",
              "      <td>0.192373</td>\n",
              "      <td>0.195840</td>\n",
              "      <td>0.194437</td>\n",
              "      <td>0.196929</td>\n",
              "      <td>0.193218</td>\n",
              "      <td>0.175780</td>\n",
              "      <td>0.128152</td>\n",
              "      <td>0.089850</td>\n",
              "      <td>0.083352</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083259</td>\n",
              "      <td>0.085485</td>\n",
              "      <td>0.109178</td>\n",
              "      <td>0.145949</td>\n",
              "      <td>0.147400</td>\n",
              "      <td>0.147373</td>\n",
              "      <td>0.153215</td>\n",
              "      <td>0.136331</td>\n",
              "      <td>0.128650</td>\n",
              "      <td>0.127448</td>\n",
              "      <td>0.124526</td>\n",
              "      <td>0.123044</td>\n",
              "      <td>0.123262</td>\n",
              "      <td>0.126940</td>\n",
              "      <td>0.126231</td>\n",
              "      <td>0.126898</td>\n",
              "      <td>0.121795</td>\n",
              "      <td>0.099420</td>\n",
              "      <td>0.084719</td>\n",
              "      <td>0.083252</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083241</td>\n",
              "      <td>0.084892</td>\n",
              "      <td>0.112470</td>\n",
              "      <td>0.192035</td>\n",
              "      <td>0.219843</td>\n",
              "      <td>0.187999</td>\n",
              "      <td>0.180500</td>\n",
              "      <td>0.186738</td>\n",
              "      <td>0.186549</td>\n",
              "      <td>0.189735</td>\n",
              "      <td>0.188186</td>\n",
              "      <td>0.190577</td>\n",
              "      <td>0.193022</td>\n",
              "      <td>0.192217</td>\n",
              "      <td>0.186904</td>\n",
              "      <td>0.186196</td>\n",
              "      <td>0.171629</td>\n",
              "      <td>0.112510</td>\n",
              "      <td>0.085044</td>\n",
              "      <td>0.083240</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083305</td>\n",
              "      <td>0.089308</td>\n",
              "      <td>0.159444</td>\n",
              "      <td>0.323082</td>\n",
              "      <td>0.435133</td>\n",
              "      <td>0.479868</td>\n",
              "      <td>0.425607</td>\n",
              "      <td>0.406373</td>\n",
              "      <td>0.416591</td>\n",
              "      <td>0.418254</td>\n",
              "      <td>0.416060</td>\n",
              "      <td>0.420284</td>\n",
              "      <td>...</td>\n",
              "      <td>0.350737</td>\n",
              "      <td>0.328330</td>\n",
              "      <td>0.313045</td>\n",
              "      <td>0.315160</td>\n",
              "      <td>0.312431</td>\n",
              "      <td>0.316695</td>\n",
              "      <td>0.318328</td>\n",
              "      <td>0.278893</td>\n",
              "      <td>0.166274</td>\n",
              "      <td>0.092502</td>\n",
              "      <td>0.083354</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083301</td>\n",
              "      <td>0.089128</td>\n",
              "      <td>0.159247</td>\n",
              "      <td>0.262119</td>\n",
              "      <td>0.234541</td>\n",
              "      <td>0.216824</td>\n",
              "      <td>0.201565</td>\n",
              "      <td>0.194039</td>\n",
              "      <td>0.190225</td>\n",
              "      <td>0.190590</td>\n",
              "      <td>0.191628</td>\n",
              "      <td>0.185784</td>\n",
              "      <td>0.175968</td>\n",
              "      <td>0.180713</td>\n",
              "      <td>0.180095</td>\n",
              "      <td>0.184120</td>\n",
              "      <td>0.171356</td>\n",
              "      <td>0.119763</td>\n",
              "      <td>0.086424</td>\n",
              "      <td>0.083276</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083240</td>\n",
              "      <td>0.084891</td>\n",
              "      <td>0.108390</td>\n",
              "      <td>0.155804</td>\n",
              "      <td>0.162704</td>\n",
              "      <td>0.179060</td>\n",
              "      <td>0.174836</td>\n",
              "      <td>0.160696</td>\n",
              "      <td>0.163860</td>\n",
              "      <td>0.181359</td>\n",
              "      <td>0.174879</td>\n",
              "      <td>0.160358</td>\n",
              "      <td>0.162727</td>\n",
              "      <td>0.175161</td>\n",
              "      <td>0.189507</td>\n",
              "      <td>0.215191</td>\n",
              "      <td>0.226223</td>\n",
              "      <td>0.136367</td>\n",
              "      <td>0.086611</td>\n",
              "      <td>0.083256</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083272</td>\n",
              "      <td>0.087915</td>\n",
              "      <td>0.154537</td>\n",
              "      <td>0.309475</td>\n",
              "      <td>0.352005</td>\n",
              "      <td>0.328503</td>\n",
              "      <td>0.358213</td>\n",
              "      <td>0.373566</td>\n",
              "      <td>0.346785</td>\n",
              "      <td>0.339945</td>\n",
              "      <td>0.390837</td>\n",
              "      <td>0.392600</td>\n",
              "      <td>...</td>\n",
              "      <td>0.332288</td>\n",
              "      <td>0.331581</td>\n",
              "      <td>0.343050</td>\n",
              "      <td>0.345295</td>\n",
              "      <td>0.330323</td>\n",
              "      <td>0.323152</td>\n",
              "      <td>0.320105</td>\n",
              "      <td>0.279885</td>\n",
              "      <td>0.169279</td>\n",
              "      <td>0.092595</td>\n",
              "      <td>0.083367</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083344</td>\n",
              "      <td>0.091139</td>\n",
              "      <td>0.171438</td>\n",
              "      <td>0.296291</td>\n",
              "      <td>0.282717</td>\n",
              "      <td>0.251271</td>\n",
              "      <td>0.217253</td>\n",
              "      <td>0.197555</td>\n",
              "      <td>0.193422</td>\n",
              "      <td>0.184207</td>\n",
              "      <td>0.180436</td>\n",
              "      <td>0.170264</td>\n",
              "      <td>0.177799</td>\n",
              "      <td>0.194378</td>\n",
              "      <td>0.195019</td>\n",
              "      <td>0.178956</td>\n",
              "      <td>0.169096</td>\n",
              "      <td>0.118785</td>\n",
              "      <td>0.086534</td>\n",
              "      <td>0.083280</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083241</td>\n",
              "      <td>0.084968</td>\n",
              "      <td>0.108683</td>\n",
              "      <td>0.163460</td>\n",
              "      <td>0.192673</td>\n",
              "      <td>0.178441</td>\n",
              "      <td>0.165327</td>\n",
              "      <td>0.172069</td>\n",
              "      <td>0.186495</td>\n",
              "      <td>0.178956</td>\n",
              "      <td>0.167983</td>\n",
              "      <td>0.162857</td>\n",
              "      <td>0.169700</td>\n",
              "      <td>0.184739</td>\n",
              "      <td>0.207286</td>\n",
              "      <td>0.224597</td>\n",
              "      <td>0.247589</td>\n",
              "      <td>0.140882</td>\n",
              "      <td>0.086850</td>\n",
              "      <td>0.083257</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083273</td>\n",
              "      <td>0.088753</td>\n",
              "      <td>0.156897</td>\n",
              "      <td>0.300279</td>\n",
              "      <td>0.347244</td>\n",
              "      <td>0.385530</td>\n",
              "      <td>0.391457</td>\n",
              "      <td>0.352362</td>\n",
              "      <td>0.352138</td>\n",
              "      <td>0.404086</td>\n",
              "      <td>0.410804</td>\n",
              "      <td>0.362560</td>\n",
              "      <td>...</td>\n",
              "      <td>0.364815</td>\n",
              "      <td>0.359175</td>\n",
              "      <td>0.341390</td>\n",
              "      <td>0.352469</td>\n",
              "      <td>0.337021</td>\n",
              "      <td>0.346255</td>\n",
              "      <td>0.342502</td>\n",
              "      <td>0.302029</td>\n",
              "      <td>0.177949</td>\n",
              "      <td>0.093565</td>\n",
              "      <td>0.083382</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083307</td>\n",
              "      <td>0.089709</td>\n",
              "      <td>0.165956</td>\n",
              "      <td>0.278964</td>\n",
              "      <td>0.248794</td>\n",
              "      <td>0.222636</td>\n",
              "      <td>0.206214</td>\n",
              "      <td>0.199162</td>\n",
              "      <td>0.194352</td>\n",
              "      <td>0.190506</td>\n",
              "      <td>0.195654</td>\n",
              "      <td>0.206230</td>\n",
              "      <td>0.195324</td>\n",
              "      <td>0.191298</td>\n",
              "      <td>0.182867</td>\n",
              "      <td>0.184448</td>\n",
              "      <td>0.180449</td>\n",
              "      <td>0.122997</td>\n",
              "      <td>0.086934</td>\n",
              "      <td>0.083285</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083243</td>\n",
              "      <td>0.085150</td>\n",
              "      <td>0.125502</td>\n",
              "      <td>0.223968</td>\n",
              "      <td>0.188510</td>\n",
              "      <td>0.181894</td>\n",
              "      <td>0.181329</td>\n",
              "      <td>0.172335</td>\n",
              "      <td>0.169007</td>\n",
              "      <td>0.173832</td>\n",
              "      <td>0.184832</td>\n",
              "      <td>0.188771</td>\n",
              "      <td>0.204627</td>\n",
              "      <td>0.220360</td>\n",
              "      <td>0.238141</td>\n",
              "      <td>0.270100</td>\n",
              "      <td>0.229637</td>\n",
              "      <td>0.128614</td>\n",
              "      <td>0.086290</td>\n",
              "      <td>0.083253</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083290</td>\n",
              "      <td>0.089593</td>\n",
              "      <td>0.165005</td>\n",
              "      <td>0.366662</td>\n",
              "      <td>0.548632</td>\n",
              "      <td>0.433108</td>\n",
              "      <td>0.428869</td>\n",
              "      <td>0.439226</td>\n",
              "      <td>0.422466</td>\n",
              "      <td>0.382484</td>\n",
              "      <td>0.364961</td>\n",
              "      <td>0.389379</td>\n",
              "      <td>...</td>\n",
              "      <td>0.356768</td>\n",
              "      <td>0.350259</td>\n",
              "      <td>0.345512</td>\n",
              "      <td>0.349427</td>\n",
              "      <td>0.360185</td>\n",
              "      <td>0.365872</td>\n",
              "      <td>0.349878</td>\n",
              "      <td>0.325673</td>\n",
              "      <td>0.188289</td>\n",
              "      <td>0.094558</td>\n",
              "      <td>0.083407</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083289</td>\n",
              "      <td>0.087291</td>\n",
              "      <td>0.130107</td>\n",
              "      <td>0.201024</td>\n",
              "      <td>0.221397</td>\n",
              "      <td>0.227356</td>\n",
              "      <td>0.207675</td>\n",
              "      <td>0.190300</td>\n",
              "      <td>0.199108</td>\n",
              "      <td>0.204295</td>\n",
              "      <td>0.192606</td>\n",
              "      <td>0.190277</td>\n",
              "      <td>0.187772</td>\n",
              "      <td>0.186235</td>\n",
              "      <td>0.187462</td>\n",
              "      <td>0.198557</td>\n",
              "      <td>0.191787</td>\n",
              "      <td>0.125617</td>\n",
              "      <td>0.087338</td>\n",
              "      <td>0.083290</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 677 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          0         1         2         3  ...       673       674       675  clase\n",
              "0  0.083223  0.083223  0.083223  0.083225  ...  0.083223  0.083223  0.083223      0\n",
              "1  0.083223  0.083223  0.083223  0.083241  ...  0.083223  0.083223  0.083223      0\n",
              "2  0.083223  0.083223  0.083223  0.083240  ...  0.083223  0.083223  0.083223      0\n",
              "3  0.083223  0.083223  0.083223  0.083241  ...  0.083223  0.083223  0.083223      0\n",
              "4  0.083223  0.083223  0.083223  0.083243  ...  0.083223  0.083223  0.083223      0\n",
              "\n",
              "[5 rows x 677 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQZCWzLks2Sj"
      },
      "source": [
        "X = datos.drop('clase', axis=1)\n",
        "Y = datos['clase']"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xtxnY3PtILA"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, \n",
        "                                                    shuffle=True, random_state=0)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMXmutUb2m-t"
      },
      "source": [
        "# 1. Construcción del modelo "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPZ68wASog_I"
      },
      "source": [
        "* [Guia TF del Modelo secuencial](https://www.tensorflow.org/guide/keras/sequential_model)\n",
        "* [Funciones de activación ](https://keras.io/api/layers/activations/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0VcwB3xxFCZ"
      },
      "source": [
        "El codigo para entrenar un perceptron simple usando TensorFlow/Keras, que ha sido diseñado principalmente para arquitecturas de redes neuronales profundas con diferentes funciones de activación que resultan en gradientes diferentes de cero,  la funcion de activación original del perceptron (que es una función  no continua en el cero), se remplaza por hard sigmoid."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3IKyzTCDNGo"
      },
      "source": [
        "model = tf.keras.models.Sequential([               \n",
        "\n",
        "  tf.keras.layers.Dense(1,                                    # dimension de la salida\n",
        "                        input_shape = (676,),                 # dimensiones de la entrada       \n",
        "                        activation='hard_sigmoid',            # en tf/keras para perceptrón    \n",
        "                        kernel_initializer ='glorot_uniform'  # para inicializar pesos de forma aleatoria y que no sean ceros\n",
        "                         )\n",
        "\n",
        "  #Dropout previene el sobreajuste del modelo haciendo 0 un porcentaje de las entradas.\n",
        "  #tf.keras.layers.Dropout(0.2),  \n",
        "\n",
        "])"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfZQ-CvkwJs3"
      },
      "source": [
        "\n",
        "En el parámetro  `kernel_initializer ` se establece 'glorot_uniform' ya que el entrenamiento será con stochastic gradient descent (en lugar del la regla de aprendizaje del perceptron) por lo que es necesario inicializar los pesos con pesos aleatorios que no sean ceros."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYBtK0CqY_jM",
        "outputId": "25af5bc3-bfd7-42e9-f083-6eebfadbf571"
      },
      "source": [
        "#Regresa una lista de las capas del modelo\n",
        "model.layers"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tensorflow.python.keras.layers.core.Dense at 0x7f9c680fa110>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwTT7x5bbhxD",
        "outputId": "9b54367c-d774-41f6-c810-7c7eb12c534a"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_2 (Dense)              (None, 1)                 677       \n",
            "=================================================================\n",
            "Total params: 677\n",
            "Trainable params: 677\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eT3z0t0_aZwa",
        "outputId": "7aeb1dd6-66ca-4d91-ced0-ade63cc24ecd"
      },
      "source": [
        "model.add(tf.keras.layers.Dense(3,\n",
        "                                activation='relu'))\n",
        "model.summary()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_2 (Dense)              (None, 1)                 677       \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 3)                 6         \n",
            "=================================================================\n",
            "Total params: 683\n",
            "Trainable params: 683\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqHHVV1ANtsQ",
        "outputId": "9f17ecda-e3d2-45c6-d378-296c9fc1dbbb"
      },
      "source": [
        "model.pop()\n",
        "model.summary()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_2 (Dense)              (None, 1)                 677       \n",
            "=================================================================\n",
            "Total params: 677\n",
            "Trainable params: 677\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        },
        "id": "BwvdM7vTO2m0",
        "outputId": "f9995278-6dde-476b-9606-7e707e317bac"
      },
      "source": [
        "tf.keras.utils.plot_model( \n",
        "    model,\n",
        "    to_file=\"model.png\",\n",
        "    show_shapes=False,\n",
        "    show_dtype=False,\n",
        "    show_layer_names=True,\n",
        "    rankdir=\"LR\",\n",
        "    dpi=96,\n",
        ")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEj4kqqjNfRW"
      },
      "source": [
        "# Entrenamiento y validación del modelo\n",
        "\n",
        "Compilar el modelo eligiendo un optimizador"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXPqMdR4NbeQ"
      },
      "source": [
        "model.compile(loss='binary_crossentropy',       # función objetivo  que se busca minimizar\n",
        "                                                # https://keras.io/api/losses/\n",
        "              \n",
        "              optimizer='adam',                 # stochastic gradient descent\n",
        "                                                # https://keras.io/api/optimizers/adam/\n",
        "              \n",
        "              metrics=['accuracy'])             \n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81D05Qk_UoCZ",
        "outputId": "420f9ad8-ea16-4cbf-8c6b-9ae26cca81a8"
      },
      "source": [
        "history = model.fit(x_train, y_train,    \n",
        "                    epochs=60,                 \n",
        "                    batch_size=250,       \n",
        "                    verbose = 2,       \n",
        "                    validation_data = (x_test, y_test))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "15/15 - 1s - loss: 0.7305 - accuracy: 0.5384 - val_loss: 0.6374 - val_accuracy: 0.7196\n",
            "Epoch 2/60\n",
            "15/15 - 0s - loss: 0.6314 - accuracy: 0.7051 - val_loss: 0.6074 - val_accuracy: 0.7182\n",
            "Epoch 3/60\n",
            "15/15 - 0s - loss: 0.6195 - accuracy: 0.7017 - val_loss: 0.5901 - val_accuracy: 0.7189\n",
            "Epoch 4/60\n",
            "15/15 - 0s - loss: 0.6026 - accuracy: 0.7057 - val_loss: 0.5823 - val_accuracy: 0.7196\n",
            "Epoch 5/60\n",
            "15/15 - 0s - loss: 0.5944 - accuracy: 0.7054 - val_loss: 0.5769 - val_accuracy: 0.7196\n",
            "Epoch 6/60\n",
            "15/15 - 0s - loss: 0.5883 - accuracy: 0.7057 - val_loss: 0.5712 - val_accuracy: 0.7196\n",
            "Epoch 7/60\n",
            "15/15 - 0s - loss: 0.5837 - accuracy: 0.7057 - val_loss: 0.5667 - val_accuracy: 0.7189\n",
            "Epoch 8/60\n",
            "15/15 - 0s - loss: 0.5789 - accuracy: 0.7057 - val_loss: 0.5616 - val_accuracy: 0.7196\n",
            "Epoch 9/60\n",
            "15/15 - 0s - loss: 0.5746 - accuracy: 0.7057 - val_loss: 0.5573 - val_accuracy: 0.7189\n",
            "Epoch 10/60\n",
            "15/15 - 0s - loss: 0.5699 - accuracy: 0.7057 - val_loss: 0.5535 - val_accuracy: 0.7189\n",
            "Epoch 11/60\n",
            "15/15 - 0s - loss: 0.5659 - accuracy: 0.7051 - val_loss: 0.5495 - val_accuracy: 0.7189\n",
            "Epoch 12/60\n",
            "15/15 - 0s - loss: 0.5622 - accuracy: 0.7049 - val_loss: 0.5460 - val_accuracy: 0.7189\n",
            "Epoch 13/60\n",
            "15/15 - 0s - loss: 0.5588 - accuracy: 0.7049 - val_loss: 0.5437 - val_accuracy: 0.7163\n",
            "Epoch 14/60\n",
            "15/15 - 0s - loss: 0.5558 - accuracy: 0.7049 - val_loss: 0.5539 - val_accuracy: 0.7182\n",
            "Epoch 15/60\n",
            "15/15 - 0s - loss: 0.5523 - accuracy: 0.7049 - val_loss: 0.5389 - val_accuracy: 0.7202\n",
            "Epoch 16/60\n",
            "15/15 - 0s - loss: 0.5504 - accuracy: 0.7060 - val_loss: 0.5373 - val_accuracy: 0.7202\n",
            "Epoch 17/60\n",
            "15/15 - 0s - loss: 0.5494 - accuracy: 0.7080 - val_loss: 0.5476 - val_accuracy: 0.7202\n",
            "Epoch 18/60\n",
            "15/15 - 0s - loss: 0.5459 - accuracy: 0.7097 - val_loss: 0.5400 - val_accuracy: 0.7202\n",
            "Epoch 19/60\n",
            "15/15 - 0s - loss: 0.5443 - accuracy: 0.7097 - val_loss: 0.5381 - val_accuracy: 0.7288\n",
            "Epoch 20/60\n",
            "15/15 - 0s - loss: 0.5422 - accuracy: 0.7085 - val_loss: 0.5365 - val_accuracy: 0.7307\n",
            "Epoch 21/60\n",
            "15/15 - 0s - loss: 0.5403 - accuracy: 0.7142 - val_loss: 0.5484 - val_accuracy: 0.7288\n",
            "Epoch 22/60\n",
            "15/15 - 0s - loss: 0.5394 - accuracy: 0.7192 - val_loss: 0.5477 - val_accuracy: 0.7301\n",
            "Epoch 23/60\n",
            "15/15 - 0s - loss: 0.5410 - accuracy: 0.7212 - val_loss: 0.5334 - val_accuracy: 0.7380\n",
            "Epoch 24/60\n",
            "15/15 - 0s - loss: 0.5509 - accuracy: 0.7122 - val_loss: 0.5379 - val_accuracy: 0.7492\n",
            "Epoch 25/60\n",
            "15/15 - 0s - loss: 0.5381 - accuracy: 0.7280 - val_loss: 0.5360 - val_accuracy: 0.7538\n",
            "Epoch 26/60\n",
            "15/15 - 0s - loss: 0.8378 - accuracy: 0.4534 - val_loss: 0.7934 - val_accuracy: 0.4068\n",
            "Epoch 27/60\n",
            "15/15 - 0s - loss: 0.6496 - accuracy: 0.6258 - val_loss: 0.5446 - val_accuracy: 0.7505\n",
            "Epoch 28/60\n",
            "15/15 - 0s - loss: 0.5504 - accuracy: 0.7263 - val_loss: 0.5485 - val_accuracy: 0.7314\n",
            "Epoch 29/60\n",
            "15/15 - 0s - loss: 0.5508 - accuracy: 0.7167 - val_loss: 0.5381 - val_accuracy: 0.7386\n",
            "Epoch 30/60\n",
            "15/15 - 0s - loss: 0.5471 - accuracy: 0.7274 - val_loss: 0.5315 - val_accuracy: 0.7426\n",
            "Epoch 31/60\n",
            "15/15 - 0s - loss: 0.5468 - accuracy: 0.7302 - val_loss: 0.5308 - val_accuracy: 0.7406\n",
            "Epoch 32/60\n",
            "15/15 - 0s - loss: 0.5463 - accuracy: 0.7274 - val_loss: 0.5370 - val_accuracy: 0.7354\n",
            "Epoch 33/60\n",
            "15/15 - 0s - loss: 0.5463 - accuracy: 0.7238 - val_loss: 0.5365 - val_accuracy: 0.7360\n",
            "Epoch 34/60\n",
            "15/15 - 0s - loss: 0.5454 - accuracy: 0.7263 - val_loss: 0.5297 - val_accuracy: 0.7393\n",
            "Epoch 35/60\n",
            "15/15 - 0s - loss: 0.5450 - accuracy: 0.7269 - val_loss: 0.5293 - val_accuracy: 0.7400\n",
            "Epoch 36/60\n",
            "15/15 - 0s - loss: 0.5448 - accuracy: 0.7266 - val_loss: 0.5356 - val_accuracy: 0.7380\n",
            "Epoch 37/60\n",
            "15/15 - 0s - loss: 0.5441 - accuracy: 0.7269 - val_loss: 0.5286 - val_accuracy: 0.7406\n",
            "Epoch 38/60\n",
            "15/15 - 0s - loss: 0.5438 - accuracy: 0.7277 - val_loss: 0.5281 - val_accuracy: 0.7426\n",
            "Epoch 39/60\n",
            "15/15 - 0s - loss: 0.5434 - accuracy: 0.7277 - val_loss: 0.5285 - val_accuracy: 0.7400\n",
            "Epoch 40/60\n",
            "15/15 - 0s - loss: 0.5431 - accuracy: 0.7269 - val_loss: 0.5342 - val_accuracy: 0.7393\n",
            "Epoch 41/60\n",
            "15/15 - 0s - loss: 0.5427 - accuracy: 0.7263 - val_loss: 0.5337 - val_accuracy: 0.7360\n",
            "Epoch 42/60\n",
            "15/15 - 0s - loss: 0.5426 - accuracy: 0.7269 - val_loss: 0.5292 - val_accuracy: 0.7400\n",
            "Epoch 43/60\n",
            "15/15 - 0s - loss: 0.5420 - accuracy: 0.7255 - val_loss: 0.5331 - val_accuracy: 0.7360\n",
            "Epoch 44/60\n",
            "15/15 - 0s - loss: 0.5418 - accuracy: 0.7263 - val_loss: 0.5330 - val_accuracy: 0.7406\n",
            "Epoch 45/60\n",
            "15/15 - 0s - loss: 0.5412 - accuracy: 0.7277 - val_loss: 0.5262 - val_accuracy: 0.7413\n",
            "Epoch 46/60\n",
            "15/15 - 0s - loss: 0.5409 - accuracy: 0.7271 - val_loss: 0.5259 - val_accuracy: 0.7419\n",
            "Epoch 47/60\n",
            "15/15 - 0s - loss: 0.5405 - accuracy: 0.7269 - val_loss: 0.5322 - val_accuracy: 0.7413\n",
            "Epoch 48/60\n",
            "15/15 - 0s - loss: 0.5402 - accuracy: 0.7266 - val_loss: 0.5262 - val_accuracy: 0.7426\n",
            "Epoch 49/60\n",
            "15/15 - 0s - loss: 0.5398 - accuracy: 0.7271 - val_loss: 0.5317 - val_accuracy: 0.7393\n",
            "Epoch 50/60\n",
            "15/15 - 0s - loss: 0.5397 - accuracy: 0.7255 - val_loss: 0.5316 - val_accuracy: 0.7433\n",
            "Epoch 51/60\n",
            "15/15 - 0s - loss: 0.5392 - accuracy: 0.7257 - val_loss: 0.5245 - val_accuracy: 0.7465\n",
            "Epoch 52/60\n",
            "15/15 - 0s - loss: 0.5388 - accuracy: 0.7263 - val_loss: 0.5251 - val_accuracy: 0.7446\n",
            "Epoch 53/60\n",
            "15/15 - 0s - loss: 0.5387 - accuracy: 0.7274 - val_loss: 0.5251 - val_accuracy: 0.7439\n",
            "Epoch 54/60\n",
            "15/15 - 0s - loss: 0.5382 - accuracy: 0.7263 - val_loss: 0.5311 - val_accuracy: 0.7419\n",
            "Epoch 55/60\n",
            "15/15 - 0s - loss: 0.5354 - accuracy: 0.7288 - val_loss: 0.5243 - val_accuracy: 0.7512\n",
            "Epoch 56/60\n",
            "15/15 - 0s - loss: 0.5375 - accuracy: 0.7302 - val_loss: 0.5637 - val_accuracy: 0.7386\n",
            "Epoch 57/60\n",
            "15/15 - 0s - loss: 0.6370 - accuracy: 0.6589 - val_loss: 0.6252 - val_accuracy: 0.6827\n",
            "Epoch 58/60\n",
            "15/15 - 0s - loss: 0.5724 - accuracy: 0.7156 - val_loss: 0.5238 - val_accuracy: 0.7518\n",
            "Epoch 59/60\n",
            "15/15 - 0s - loss: 0.5393 - accuracy: 0.7240 - val_loss: 0.5446 - val_accuracy: 0.7380\n",
            "Epoch 60/60\n",
            "15/15 - 0s - loss: 0.5330 - accuracy: 0.7257 - val_loss: 0.5233 - val_accuracy: 0.7512\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7dTAzgHDUh7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57e96611-e6a3-49c0-8ab2-64c791fbafb0"
      },
      "source": [
        "score = model.evaluate(x_test,  y_test, verbose=2)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "48/48 - 0s - loss: 0.5233 - accuracy: 0.7512\n",
            "Test loss: 0.5233256220817566\n",
            "Test accuracy: 0.7511520981788635\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgAydS_U1OVC"
      },
      "source": [
        "Referencias: \n",
        "1. Training the Perceptron with Scikit-Learn and TensorFlow | QuantStart [Internet]. [citado 19 de julio de 2021]. Disponible en: https://www.quantstart.com/articles/training-the-perceptron-with-scikit-learn-and-tensorflow/\n"
      ]
    }
  ]
}